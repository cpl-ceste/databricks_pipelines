{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e79ed77-31e4-47db-a9f9-b38bfc5f2c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sesion 2 - Técnicas de Optimización en Spark y Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a23bdcb-ff70-4d5d-beab-94afb0dca149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 1 – Buenas prácticas de código Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d46881-8da1-4c02-ba06-ec1a88290036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Antes de pensar en configuraciones avanzadas, la **primera fuente de optimización** está en cómo escribimos el código Spark.\n",
    "\n",
    "Malas prácticas → pipelines lentos, costosos e inestables.  \n",
    "Buenas prácticas → rendimiento más alto, menos shuffles, menos memoria usada.\n",
    "\n",
    "En este bloque veremos:\n",
    "- Selección de columnas vs `select *`.\n",
    "- Pushdown de filtros (filtrar lo antes posible).\n",
    "- Evitar operaciones caras innecesarias (joins, collect, count repetidos).\n",
    "- Uso consciente de cache/persist.\n",
    "- Reutilización de DataFrames vs recomputación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6261aa1e-4c80-4350-9110-8e307fc183b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Selección de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa1352d0-f760-4fe1-bd67-f531be5c5696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "❌ Mala práctica: `select *` en datasets grandes.  \n",
    "✅ Buena práctica: seleccionar solo las columnas necesarias.\n",
    "\n",
    "Esto impacta en:\n",
    "- **E/S de datos** → menos lectura de disco.  \n",
    "- **Memoria** → menos columnas almacenadas en RAM.  \n",
    "- **Shuffles** → menos datos transferidos entre nodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc566e9a-67b4-475f-aeb5-1f5f6923821a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c3a6a1-605b-4747-8f10-4c7f6cd89d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Dataset de ejemplo\n",
    "df_vuelos = (\n",
    "    spark.range(0, 10_000_000)\n",
    "    .withColumn(\"origen\", F.when(F.col(\"id\") % 3 == 0, \"MAD\").otherwise(\"BCN\"))\n",
    "    .withColumn(\"destino\", F.when(F.col(\"id\") % 5 == 0, \"JFK\").otherwise(\"CDG\"))\n",
    "    .withColumn(\"delay\", (F.rand() * 300).cast(\"int\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07df87d2-f280-4db3-915f-f1609ff6bf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ❌ select * (todas las columnas)\n",
    "print(\"Select *\")\n",
    "%time print(df_vuelos.select(\"*\").count())\n",
    "\n",
    "# ✅ solo columnas necesarias\n",
    "print(\"Select columnas necesarias\")\n",
    "%time print(df_vuelos.select(\"id\", \"delay\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcb36159-e692-4e67-9b93-05dc23945168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Pushdown de filtros (filtrar antes, no después)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d190c5-d2df-49f9-ad96-3385cb399e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "❌ Mala práctica: aplicar filtros tarde (después de joins, groupBy, etc.).  \n",
    "✅ Buena práctica: filtrar lo antes posible.\n",
    "\n",
    "Beneficios:\n",
    "- Menos filas procesadas en etapas posteriores.\n",
    "- Menos datos que mover entre particiones.\n",
    "- Queries más rápidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7953c58-c0f3-49c0-87bd-8a166b9d8dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Dataset de ejemplo\n",
    "df_vuelos = spark.range(0, 50_000_000).withColumn(\"origen\", (F.rand()*10).cast(\"int\")) \\\n",
    "                                      .withColumn(\"delay\", (F.rand()*500).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b17045-708e-4c9d-8cc9-850baa2d1d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ❌ Filtro tarde (después del groupBy)\n",
    "df_tarde = df_vuelos.groupBy(\"origen\").count().filter(\"count > 1000000\")\n",
    "\n",
    "# Ejecución y tiempos\n",
    "print(\"Filtro tarde:\")\n",
    "%time df_tarde.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeb9306b-c66c-48b8-969d-f8a9ae11a565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Filtro temprano (antes del groupBy)\n",
    "df_temprano = df_vuelos.filter(\"delay > 200\").groupBy(\"origen\").count()\n",
    "\n",
    "print(\"Filtro temprano:\")\n",
    "%time df_temprano.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42934c61-b13a-493f-b225-1b2d444e7be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Plan Filtro tarde ===\")\n",
    "df_tarde.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d21d1843-e3cc-4422-9382-b5b9bb5c2edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Plan Filtro temprano ===\")\n",
    "df_temprano.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd9d3bb-8e0a-4c0d-b418-669990e15172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- En `df_tarde` (filtro tarde):\n",
    "El filtro aparece después del Aggregate. Spark primero hace el groupBy, después reduce los resultados → mucho más costoso.\n",
    "\n",
    "- En `df_temprano` (filtro temprano):\n",
    "El filtro aparece antes del Aggregate. Spark reduce los datos antes del shuffle → más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba1f6ff-754b-4c92-b06c-c905a473305c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Joins optimizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a69da7-f94d-4bce-ac73-26c25420d981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Los joins pueden ser el mayor cuello de botella.\n",
    "\n",
    "- **Shuffle join (por defecto)** → mueve datos entre nodos, costoso.\n",
    "- **Broadcast join** → si una tabla cabe en memoria, se copia a todos los nodos. Muy rápido.\n",
    "- **Cross join** → prohibido salvo casos muy controlados.\n",
    "\n",
    "Regla: siempre que tengas una tabla pequeña, usa `broadcast()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33431a71-1565-4d4c-89b6-ba52521a0326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Dataset grande de vuelos\n",
    "df_vuelos = spark.range(0, 20_000_000) \\\n",
    "    .withColumn(\"origen\", (F.rand()*10).cast(\"int\")) \\\n",
    "    .withColumn(\"delay\", (F.rand()*500).cast(\"int\"))\n",
    "\n",
    "# Dataset pequeño de aeropuertos\n",
    "df_aeropuertos = spark.createDataFrame(\n",
    "    [\n",
    "        (\"0\", \"MAD\"), (\"1\", \"BCN\"), (\"2\", \"JFK\"),\n",
    "        (\"3\", \"CDG\"), (\"4\", \"MEX\"), (\"5\", \"SCL\")\n",
    "    ],\n",
    "    [\"codigo\", \"ciudad\"]\n",
    ").withColumn(\"codigo\", F.col(\"codigo\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15e824c-425f-498b-9447-bcf65bc13fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ❌ Join normal (shuffle join)\n",
    "df_join_normal = df_vuelos.join(df_aeropuertos, df_vuelos.origen == df_aeropuertos.codigo)\n",
    "\n",
    "# Ejecución\n",
    "print(\"Join normal (con shuffle):\")\n",
    "%time df_join_normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a423d67-5efd-41da-959c-b959ce7cf792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ✅ Join con broadcast (spark replica df_aeropuertos en cada executor)\n",
    "df_join_broadcast = df_vuelos.join(F.broadcast(df_aeropuertos), df_vuelos.origen == df_aeropuertos.codigo)\n",
    "\n",
    "print(\"Join con broadcast:\")\n",
    "%time df_join_broadcast.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3cd3629-d881-46d8-9d2c-7ea1fbdfaa0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "🔎 Qué esperar en tiempos:\n",
    "\n",
    " - Join normal → Spark hace un shuffle grande de df_vuelos y df_aeropuertos.\n",
    " - Join broadcast → Spark replica la tabla pequeña en cada executor y evita shuffle → más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c75408-5584-4568-bf0d-fc238fa92805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Plan join normal ===\")\n",
    "df_join_normal.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0110d7-459d-4fe7-b8c6-d6624fa5ea06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Plan join broadcast ===\")\n",
    "df_join_broadcast.explain(\"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ce96aa3-d9f4-4726-972d-dc33f35f39c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿En qué escenario usarías broadcast join en producción?\n",
    "\n",
    "✅ Cuando:\n",
    "- Una de las tablas es pequeña (cientos de MB como máximo, normalmente < 10^8 filas).\n",
    "- La otra tabla es muy grande (miles de millones de filas).\n",
    "- Quieres evitar un shuffle costoso.\n",
    "\n",
    "❌ Evitar broadcast si:\n",
    "- La tabla pequeña no cabe en memoria.\n",
    "- Hay muchas ejecuciones concurrentes → replicar la tabla puede saturar los workers.\n",
    "\n",
    "\n",
    "👉 Conclusión:\n",
    "Un broadcast join es ideal para dimension tables (ej. catálogos de aeropuertos, países, monedas) unidas a fact tables grandes (ej. vuelos, tickets, reservas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed7c678-67b2-4845-932f-435766c983b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Cache y persist con criterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79ca206-2b9a-491d-8c0c-99ebc554ffb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Spark es \"lazy\" → recalcula un DataFrame cada vez que lo usas.  \n",
    "Si vas a reutilizarlo varias veces, merece la pena **cachear**. Es decir: `cache` convierte resultados intermedios en algo reutilizable.\n",
    "\n",
    "❌ Mala práctica: cachear todo \"por si acaso\".  \n",
    "✅ Buena práctica: cachear solo cuando se reutiliza varias veces.\n",
    "\n",
    "Modos:\n",
    "- `df.cache()` → RAM.\n",
    "- `df.persist(StorageLevel.MEMORY_AND_DISK)` → RAM y disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7cbc863-8e0d-4d23-882d-26e682efc84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Dataset grande\n",
    "df_vuelos = spark.range(0, 20_000_000).withColumn(\"delay\", (F.rand()*500).cast(\"int\"))\n",
    "\n",
    "# 🚫 Sin cache\n",
    "df_delay = df_vuelos.filter(\"delay > 200\")\n",
    "\n",
    "print(\"== SIN CACHE ==\")\n",
    "t0 = time.time()\n",
    "df_delay.count()  # Primera acción\n",
    "print(\"Primera acción:\", round(time.time()-t0, 2), \"seg\")\n",
    "\n",
    "t0 = time.time()\n",
    "df_delay.agg(F.avg(\"delay\")).collect()  # Segunda acción\n",
    "print(\"Segunda acción:\", round(time.time()-t0, 2), \"seg\")\n",
    "\n",
    "\n",
    "# ✅ Con cache\n",
    "df_delay_cached = df_vuelos.filter(\"delay > 200\").cache()\n",
    "\n",
    "print(\"\\n== CON CACHE ==\")\n",
    "t0 = time.time()\n",
    "df_delay_cached.count()  # Primera acción (carga en memoria)\n",
    "print(\"Primera acción (con cache):\", round(time.time()-t0, 2), \"seg\")\n",
    "\n",
    "t0 = time.time()\n",
    "df_delay_cached.agg(F.avg(\"delay\")).collect()  # Segunda acción (ya cacheado)\n",
    "print(\"Segunda acción (con cache):\", round(time.time()-t0, 2), \"seg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57241622-231a-4ffa-a2e0-0907c60fe7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Cache en DAG](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a26f60-35ff-4253-8caa-7fc87900db6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "🔎 Qué esperar en los tiempos:\n",
    "\n",
    "- Sin cache → la segunda acción es lenta porque Spark vuelve a recalcular el plan completo (filter sobre los 20M de filas).\n",
    "- Con cache → la primera acción tarda más (porque Spark calcula + guarda en memoria), pero la segunda acción es mucho más rápida, ya que lee los datos directamente desde RAM/SSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2af77859-bbb3-47be-af9c-fba4094c297f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿Qué riesgos tiene cachear demasiados DataFrames?\n",
    "\n",
    "⚠️ Riesgos en producción:\n",
    "\n",
    "- Consumo de memoria elevado → si cacheas demasiados DataFrames grandes, puedes agotar la memoria del cluster.\n",
    "- Eviction (expulsión del cache) → Spark liberará cachés antiguos si no hay espacio → recalculando todo de nuevo, lo que puede ser peor que no cachear.\n",
    "- Persistencia incorrecta → si los datos son poco reutilizados, el overhead de mantenerlos en memoria puede ser mayor que el beneficio.\n",
    "\n",
    "👉 Buenas prácticas:\n",
    "- Cachear solo lo que realmente vas a reutilizar varias veces.\n",
    "- Usar `.unpersist()` cuando ya no lo necesites.\n",
    "- Monitorizar en Spark UI → pestaña Storage muestra qué DataFrames están cacheados, su tamaño y en qué nivel (MEMORY, DISK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10cd858c-a3fa-4f7b-bcc8-4f0f909f8d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejercicio Bloque 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ace4282-cc28-43bb-a673-3802f7f81924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "La aerolínea quiere analizar los vuelos **con más de 3 horas de retraso**.  \n",
    "\n",
    "Tareas:\n",
    "1. Filtra el DataFrame de vuelos para quedarte con esos casos.  \n",
    "2. Haz un join con la tabla de aeropuertos (usa `broadcast`).  \n",
    "3. Calcula el retraso medio por ciudad origen.  \n",
    "4. Cachea el DataFrame si lo vas a usar más de una vez.  \n",
    "5. Comprueba que tu plan físico (`explain(\"extended\")`) muestre:\n",
    "   - Filtro temprano.\n",
    "   - Broadcast join.\n",
    "   - Cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8009dd5-1961-45ce-833b-a90942acaef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Dataset de vuelos simulado\n",
    "vuelos = spark.range(0, 20_000_000) \\\n",
    "    .withColumn(\"origen\", (F.rand()*10).cast(\"int\")) \\\n",
    "    .withColumn(\"delay\", (F.rand()*400).cast(\"int\"))  # retrasos hasta 400 min\n",
    "\n",
    "# Dataset de aeropuertos (pequeño → ideal para broadcast)\n",
    "aeropuertos = spark.createDataFrame([\n",
    "    (0, \"Madrid\"), (1, \"Barcelona\"), (2, \"Valencia\"),\n",
    "    (3, \"Bilbao\"), (4, \"Sevilla\"), (5, \"Gran Canaria\"),\n",
    "    (6, \"Málaga\"), (7, \"Zaragoza\"), (8, \"Alicante\"), (9, \"Santander\")\n",
    "], [\"origen\", \"ciudad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c413228-f441-4da7-a116-43de9e6f4135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n",
    "\n",
    "## 1. Filtrar vuelos con más de 3 horas de retraso (180 minutos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d85273e-4a4c-48c1-86d2-23d03b5c6f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n",
    "\n",
    "## 2. Join con tabla de aeropuertos usando broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f746293-190b-4d57-8c7d-4d56205047c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n",
    "\n",
    "## 3. Calcular retraso medio por ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df5d669-df03-4b57-af7c-33ded4c34648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n",
    "\n",
    "## 4. Cachear el DataFrame (porque lo usaremos varias veces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6630810-dcd9-4d06-b7de-01426282e5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Guardamos el DataFrame en memoria como tabla temporal o en Delta/Lakehouse y lo leemos de nuevo. Así evitamos recalcular la query original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04408e7-1fc4-42cc-a8ce-b48a56db8342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n",
    "\n",
    "## 4.2. Simular el cache\n",
    "\n",
    "### Guardar en disco en formato Delta\n",
    "\n",
    "### Leer de nuevo (simulación de cache)\n",
    "\n",
    "### Primera acción\n",
    "\n",
    "### Segunda acción (ya sin recomputar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a9e74a-fb7d-4714-b6fc-c74912f2e7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Verificar con .explain(\"extended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd1cf60e-d4a0-45e7-aaf9-00f010e2240d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "🔎 Qué deberíamos ver en el plan físico:\n",
    "\n",
    "- Filtro temprano: el `Filter (delay > 180)` aparece justo después del Range inicial.\n",
    "- Broadcast join: Spark indica `BroadcastHashJoin`.\n",
    "- Cache: Spark mostrará un `InMemoryTableScan` en ejecuciones posteriores → señal de que lee desde cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc1d787-d85e-4a9a-a9ff-3fb1954b2051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6.1 Comprobar tiempos\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c22ebd1-42c0-4bc9-ab60-fd9dad0b2d07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6.2 Leer de nuevo (simulación de cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88dba3b5-8032-4a19-8ca3-a142516a6d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 2. Optimización de particionado y tamaño de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63836244-35dd-44f0-bb0f-064a5f38ac81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Ajuste de `spark.sql.shuffle.partitions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4b7c5a-282c-4e76-9efc-86de1e002d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revisamos el valor actual\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72cb71a-3aed-42f8-a5b8-c9be977d8704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "\n",
    "# Creamos dataset de 1 millón de filas con año entre 2010 y 2019\n",
    "data = [(i, random.randint(2010, 2019), f\"name_{i%100}\") for i in range(1, 1000001)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"year\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83277a48-a71d-4ce0-bc07-55e221194b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc511c0-b895-446e-bf7d-a71cbe665d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Función para medir tiempo\n",
    "import time\n",
    "\n",
    "def test_shuffle_partitions(n_partitions):\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", n_partitions)\n",
    "    start = time.time()\n",
    "    df.groupBy(\"name\").count().collect()\n",
    "    end = time.time()\n",
    "    print(f\"Partitions: {n_partitions}, Time: {end - start:.2f} s\")\n",
    "\n",
    "# Probamos diferentes valores\n",
    "test_shuffle_partitions(50)\n",
    "test_shuffle_partitions(200)\n",
    "test_shuffle_partitions(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e193c8cb-bf69-4e6e-8226-ed1850ae9f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "✅ Claves:\n",
    "\n",
    "- Mientras más particiones, mayor overhead de tareas pequeñas (task scheduling), pero menor tamaño de cada partición.\n",
    "- Mientras menos particiones, más riesgo de tareas grandes (skew) y uso elevado de memoria.\n",
    "- La variación se nota en datasets grandes y con operaciones de shuffle (joins, groupBy, aggregations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dba4618-408a-4f01-bc79-9c84fc301fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Particionamiento de tablas Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4ca0be-a61a-4b84-9dbf-e94a6c66a04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El particionamiento físico de Delta tables mejora las consultas que filtran por la columna de partición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a30ff20-a1f5-4849-8c22-8e2e1d16977f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos en Delta particionando por 'year'\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"year\").save(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_table_partitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dd6f6e-fa3e-4c80-a1b7-5efcd8fc16d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filtrando por la columna de partición\n",
    "\n",
    "Aquí debería hacer _partition pruning_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dd7e09-8c2e-444e-b979-f1183020f8a6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756484185637}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_part = spark.read.format(\"delta\").load(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_table_partitioned\")\n",
    "\n",
    "df_filtered_partition = df_part.filter(F.col(\"year\") == 2015) \\\n",
    "                               .withColumn(\"input_file\", F.col(\"_metadata.file_path\"))\n",
    "\n",
    "display(df_filtered_partition)\n",
    "\n",
    "print(\"Archivos leídos al filtrar por partición (year=2015):\")\n",
    "display(df_filtered_partition.select(\"input_file\").distinct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa1ab6a-33c6-475e-86c7-83054553ffef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Solo se leen archivos dentro de la partición \"n\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f7c54b-0178-4ebd-bc83-362f48e61d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Filtrando por otra columna\n",
    "\n",
    "👉 Aquí no hay pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa19d793-707c-4f0b-adfa-2aba590d7f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered_other = df_part.filter(F.col(\"name\") == \"name_42\") \\\n",
    "                           .withColumn(\"input_file\", F.col(\"_metadata.file_path\"))\n",
    "\n",
    "display(df_filtered_other)\n",
    "\n",
    "print(\"Archivos leídos al filtrar por otra columna (name):\")\n",
    "display(df_filtered_other.select(\"input_file\").distinct())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d5d969d-7c58-49b6-af18-dd8718ae12e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Se leen archivos de varias particiones, porque Spark no puede usar el particionamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "376dbe35-0c59-4ea0-bac1-0fc2ee60c281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Conclusión\n",
    "- Filtrar por la columna de partición (year) → menos archivos leídos → consulta más rápida.\n",
    "- Filtrar por otra columna (name) → Spark escanea todos los archivos → más lento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "833373e4-e76c-434e-a69b-7fa5d3968973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 3. Optimización en Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675d3bcf-0ae6-45ee-803f-2e6eb89aee89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Optimización de ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b251494-3884-4d0d-b6d6-ee755be5944f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Simulamos que hay un problema de muchos archivos pequeños"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34452ec2-b1a7-4960-b56d-306f032741b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Simulamos 5 millones de registros con varias columnas\n",
    "data = [(i, random.randint(2010, 2020), f\"airline_{random.randint(1,50)}\", random.randint(1, 1000)) \n",
    "        for i in range(1, 500001)]\n",
    "df = spark.createDataFrame(data, [\"flight_id\", \"year\", \"airline\", \"passengers\"])\n",
    "\n",
    "# Escribimos en pequeñas tandas para generar muchos archivos\n",
    "for i in range(10):\n",
    "    df.limit(50000).write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_flights_smallfiles\")\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_flights_smallfiles\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "109cc485-5ba7-43db-ba9b-658fdd0c7e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def contar_ficheros(path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    # Filtramos solo los archivos Parquet de datos (ignorar _delta_log)\n",
    "    parquet_files = [f for f in files if f.name.endswith(\".parquet\")]\n",
    "    print(f\"Número de archivos de datos: {len(parquet_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7c316f-eee4-4d3e-935d-ae732882cd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "contar_ficheros(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_flights_smallfiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece19fed-0844-4006-a3e8-3e229861e538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Compactamos todos los archivos pequeños en menos archivos grandes\n",
    "OPTIMIZE delta.`dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_flights_smallfiles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57762a01-21f6-4fd2-9dc7-7b81cc733d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "contar_ficheros(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_flights_smallfiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88606c17-1148-4991-82bd-c9c47ac28a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Z-Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a51aa7-0be0-4452-978f-fb35d61b07bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "\n",
    "# 10 millones de filas, airline con 500 valores distintos\n",
    "data = [(i, f\"airline_{random.randint(1,500)}\", random.randint(2010, 2020)) \n",
    "        for i in range(1, 1000001)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"flight_id\", \"airline\", \"year\"])\n",
    "\n",
    "# Guardamos con muchos pequeños archivos\n",
    "df.repartition(2000).write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_zorder_demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822cc8a1-f571-4bce-8564-5c45493f6b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Primero ejecutamos una query que filtre por una aerolínea en la tabla sin optimizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6cc1ef7-546b-485d-9b09-0c5505e03c0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_zorder_demo\")\n",
    "\n",
    "start = time.time()\n",
    "res1 = df.filter(F.col(\"airline\") == \"airline_250\").count()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Antes de Z-ORDER: {end-start:.2f} s, Registros encontrados: {res1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76b598e-0ebd-4305-92b5-8854f193dc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora mejoramos el layout de archivos con Z-ORDER en airline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72568da-5ac0-49c9-8731-80d5499fc30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE delta.`dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_zorder_demo`\n",
    "ZORDER BY (airline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6022f793-3c7f-46d1-9dbc-64e31efa1e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Esto reorganiza los datos físicamente en disco de modo que valores similares de airline estén más próximos → menos archivos escaneados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11bfa14-5ed8-434a-bad7-3c835d500077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_opt = spark.read.format(\"delta\").load(\"dbfs:/Volumes/optimizacion/sesion_2/volumen/delta_zorder_demo\")\n",
    "\n",
    "start = time.time()\n",
    "res2 = df_opt.filter(F.col(\"airline\") == \"airline_250\").count()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Después de Z-ORDER: {end-start:.2f} s, Registros encontrados: {res2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4f4857-19da-4886-a932-c7e9a5f19570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Misma cantidad de registros.\n",
    "- Menor tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3d933b-ac0b-44fd-a940-3cb8280e5644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Conclusion:**\n",
    "- Cuando haces consultas frecuentes por columnas con alta cardinalidad (ej. airline, customer_id, flight_number).\n",
    "- Cuando esas columnas no son buenas candidatas para particionamiento físico (ej. demasiados valores distintos).\n",
    "- Cuando necesitas reducir el número de archivos escaneados en queries selectivas.\n",
    "\n",
    "**Particionar vs. Z-ORDER**\n",
    "- Particionar físicamente → crea carpetas en disco (/year=2020/, /year=2021/). Solo conviene en columnas de baja cardinalidad (ej. year, region).\n",
    "- Z-ORDER → no crea carpetas, sino que ordena los datos dentro de los archivos Delta para mejorar localización. Ideal en columnas de alta cardinalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90085d37-e59c-4009-9adb-ffd6370a7c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Motores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5351d5e8-712b-4050-8f85-aeb7ec2fe7d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Adaptive Query Execution (AQE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c37e90ab-2ecf-46ed-ac4e-0224cdf617af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- AQE = Adaptive Query Execution.\n",
    "- Spark ajusta dinámicamente el número de particiones y la estrategia de join durante la ejecución de la query.\n",
    "- Así evita problemas como:\n",
    "    - Demasiadas particiones pequeñas → exceso de overhead.\n",
    "    - Pocas particiones grandes → skew (particiones desbalanceadas).\n",
    "- AQE ya está habilitado por defecto.\n",
    "- No se puede cambiar spark.conf, pero sí podemos observar el Query Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8c897d-4c95-4d8d-8b17-835def3b17bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e4e19f-1137-4826-bb81-345b2290a568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Forzamos una operación de shuffle grande\n",
    "# Generamos 5 millones de filas con datos de vuelos\n",
    "data = [\n",
    "    (i, random.randint(2010, 2020), f\"airline_{random.randint(1,500)}\", random.randint(50, 300))\n",
    "    for i in range(1, 5000001)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"flight_id\", \"year\", \"airline\", \"passengers\"])\n",
    "\n",
    "agg = df.groupBy(\"airline\").agg(F.sum(\"passengers\").alias(\"total_passengers\"))\n",
    "agg.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c2969a1-6d5c-4819-922a-daafe2c8f18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Si AQE entra en juego, verás que Spark reporta un número de particiones distinto al que esperarías (ajustado dinámicamente).\n",
    "- También puede que reemplace un SortMergeJoin por un BroadcastJoin si detecta que una tabla es pequeña en tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3b6c373-c7e7-4a63-9e66-bf4cb5865f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Photon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c80a9c77-1508-47be-8d37-ebeb7da99052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Photon = motor de ejecución de consultas vectorizado escrito en C++.\n",
    "- Acelera operaciones típicas de analítica (scans, filters, joins, aggregates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9649288a-de89-4dfd-883b-49cc4ab5017a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "![Cluster Photon](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Crear cluster 2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ea7c0d-2b89-44d6-bd28-0d7f80df2232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Autoscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6994b352-426c-42d8-a781-aad46dac8a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Es la función de Databricks que aumenta o reduce automáticamente el número de nodos del cluster según la carga.\n",
    "- Ayuda a equilibrar costo vs. rendimiento.\n",
    "- Pero en Databricks Free Edition no se puede habilitar autoscaling, ya que el cluster siempre tiene 1 nodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0336215-85ff-4ae5-886b-a9adfefe4685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [\n",
    "    (i, random.randint(2010, 2020), f\"airline_{random.randint(1,500)}\", random.randint(50, 300))\n",
    "    for i in range(1, 100001)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"flight_id\", \"year\", \"airline\", \"passengers\"])\n",
    "\n",
    "def heavy_query():\n",
    "    return df.groupBy(\"year\").agg(F.avg(\"passengers\")).collect()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(heavy_query) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fbd444f-8af2-403b-91aa-cdfa7aa30b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- En Free Edition: no verán escalado, pero sí notarán que varias queries corren en paralelo y tardan más porque comparten el único nodo.\n",
    "- En un cluster real con autoscaling: se habrían añadido más nodos automáticamente."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7687026365060321,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimizacion - Sesion 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
