{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd33e414-79ec-44a8-a8ec-01fcf7e6fad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#   Clase 1 – Fundamentos y diagnóstico de rendimiento\n",
    "  **Objetivo:** Comprender cómo Spark ejecuta el código y aprender a diagnosticar cuellos de botella.\n",
    "  \n",
    "  Estructura:\n",
    "  1. Arquitectura y ejecución (driver, executors, DAG)\n",
    "  2. Stages y tasks\n",
    "  3. Shuffles y data skew\n",
    "  4. Particiones y paralelismo\n",
    "  5. Herramientas de diagnóstico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c6f37ed-656a-4181-a0dd-fa0018c07077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 1 – Transformaciones vs Acciones\n",
    "  Spark es *lazy*: las transformaciones no ejecutan nada hasta que llega una acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4c370c-5934-4cc0-b5da-259f59c79fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformaciones\n",
    "- Operaciones que definen un nuevo DataFrame/RDD.\n",
    "- _Lazy evaluation_: no ejecutan nada hasta que llega una acción.\n",
    "- Son operaciones perezosas (_lazy evaluation_): no se ejecutan inmediatamente.\n",
    "- Devuelven un nuevo DataFrame/RDD con un plan de ejecución actualizado, pero sin lanzar un job.\n",
    "- Se ejecutan solo cuando se dispara una acción.\n",
    "- Pueden ser de dos tipos:\n",
    "    1. Narrow transformations: los datos de una partición se usan solo en esa misma partición.\n",
    "        - Ejemplos: `map()`, `filter()`, `select()`, `withColumn()`.\n",
    "        - Más eficientes (no requieren shuffle).\n",
    "    2. Wide transformations: requieren mover datos entre particiones → shuffle.\n",
    "        - Ejemplos: `groupBy()`, `join()`, `distinct()`, `repartition()`.\n",
    "        - Costosas en tiempo y recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a67a3b-c889-4ae0-8519-f4ff1306c5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.range(1, 100).withColumn(\"x2\", col(\"id\") * 2)  # transformación\n",
    "df_filtered = df.filter(col(\"id\") > 50)                   # transformación\n",
    "\n",
    "# Hasta aquí no se ejecuta nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57236584-d785-4546-ae7a-c860e329c367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Acciones:\n",
    "- Disparan la ejecución real del plan.\n",
    "- Devuelven un resultado al driver o escriben datos en almacenamiento.\n",
    "- Ejemplos:\n",
    "  - `collect()` → trae todos los datos al driver (⚠️ cuidado con Out Of Memory).\n",
    "  - `show()` → muestra primeras filas en consola.\n",
    "  - `count()` → cuenta filas.\n",
    "  - `write.format(\"delta\").save(...)` → guarda en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0ed58d-1760-40fa-85f5-5117903bff8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered.show()   # aquí Spark ejecuta el pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f0f6f8-8236-41cf-a6d1-9bddb138860a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23153387-c2a0-4ed9-84ef-f72f2713144e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformaciones (lazy)\n",
    "df = spark.range(1, 1000000)\n",
    "df2 = df.withColumn(\"x2\", col(\"id\") * 2)   # narrow\n",
    "df3 = df2.filter(col(\"x2\") % 5 == 0)       # narrow\n",
    "df4 = df3.groupBy((col(\"x2\") % 10)).count() # wide → shuffle\n",
    "\n",
    "# Acción (trigger)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8cea157-43aa-445f-ba4a-8dfe703dbe5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resultado:\n",
    "- Hasta `.groupBy()` solo hay transformaciones → Spark construye el DAG.\n",
    "- Al llamar `.show()`, Spark ejecuta el job, lo divide en stages y tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c91bf1d-2381-472a-9b80-ad06a1cc5449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark UI y `explain()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c75ffce4-51b4-40d6-9f1b-e0784b7d72f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark UI\n",
    "La Spark UI es la interfaz web que te muestra qué está pasando dentro de tu aplicación Spark. En Databricks puedes acceder desde la pestaña Spark UI de cada job o notebook.\n",
    "Los elementos principales son:\n",
    "1. Jobs tab\n",
    "   - Lista de trabajos disparados por acciones (count(), show(), write...).\n",
    "   - Cada job corresponde a la ejecución de un DAG.\n",
    "   - Desde aquí entras a Stages.\n",
    "2. Stages tab\n",
    "   - Muestra cómo se divide el job en stages (fases).\n",
    "   - Cada stage tiene múltiples tasks (una por partición).\n",
    "   - Métricas clave: tiempo, duración, skew, GC, I/O.\n",
    "3. Tasks tab (dentro de un stage)\n",
    "   - Detalle de cada partición ejecutada.\n",
    "   - Puedes detectar data skew: si una task tarda mucho más que las demás.\n",
    "   - Métricas de input size, shuffle read/write, memoria usada.\n",
    "4. SQL tab\n",
    "   - Muy útil si usas DataFrames/SQL.\n",
    "   - Visualiza el plan lógico y físico en forma de árbol.\n",
    "   - Identifica dónde ocurren shuffles y scans de tablas. \n",
    "5. Storage tab\n",
    "   - Muestra qué DataFrames/RDDs están cacheados.\n",
    "   - Permite ver el uso de memoria y disco para persistencia.\n",
    "\n",
    "En Databricks también tienes el DAG Viewer, un grafo visual que muestra stages y dependencias → muy útil para enseñar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42874a24-c0d6-4454-b9ad-568a61e5093b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Spark UI Jobs](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/SparkUI Jobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01df16a-de83-43f8-a323-ce75cbcb89a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![SparkUI Stages](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/SparkUI Stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83fc06b-740b-4aee-97d9-26d7ffa65fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `explained()`\n",
    "El método `.explain()` muestra cómo Spark planea ejecutar tu DataFrame.\n",
    "Opciones principales:\n",
    "- `.explain()` → plan físico resumido.\n",
    "- `.explain(\"extended\")` → plan lógico + optimizado + físico.\n",
    "- `.explain(\"cost\")` → incluye estimación de costes (filas, bytes).\n",
    "- `.explain(\"formatted\")` → salida legible en tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b731f7b7-7fc8-40ef-8c09-f4018091d1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo Practico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5529aebf-79d3-4567-8377-b8b8587cf5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear DataFrame grande\n",
    "df = spark.range(0, 10000000)\n",
    "\n",
    "# Solo definimos transformaciones (lazy)\n",
    "df_filtered = df.filter(df.id % 2 == 0)\n",
    "df_transformed = df_filtered.withColumn(\"id_squared\", df.id * df.id)\n",
    "\n",
    "# Ver plan lógico/físico\n",
    "df_transformed.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d55ed70-1d09-4327-963e-8bf278047910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso a paso en el ejemplo:\n",
    "1. PhotonRange (1)\n",
    "   - Spark crea un dataset de enteros (de 0 a 10,000,000).\n",
    "   - Está dividido en 8 splits (= particiones iniciales).\n",
    "   - Paralelización inicial.\n",
    "2. PhotonFilter (2)\n",
    "   - Filtra solo los números pares: (id % 2 = 0).\n",
    "   - Es un narrow transformation (no hay shuffle).\n",
    "3. PhotonProject (3)\n",
    "   - Calcula una nueva columna id_squared.\n",
    "   - Otra transformación ligera, todavía sin shuffle.\n",
    "4. PhotonResultStage (4)\n",
    "   - Prepara los resultados para pasarlos al driver.\n",
    "5. ColumnarToRow (5)\n",
    "   - Convierte los datos del formato columnar interno (usado por Photon) a filas normales, porque el driver no entiende el formato columnar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6e4cb3-8f91-43c8-95d4-1b57b19d7ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ahora ejecutamos una acción\n",
    "df_transformed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66df874e-91e5-426b-8b77-900913bb24b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicio 1\n",
    "  1. Genera un DataFrame con 10 millones de filas.\n",
    "  2. Aplica un `filter` y un `groupBy` con `count`.\n",
    "  3. Usa `.explain(\"extended\")` para inspeccionar el plan.\n",
    "  \n",
    "  ❓ Pregunta: ¿en qué momento se ejecuta la query realmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8ffffc-e5c6-47a1-94d8-3f8fc3eef8c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu código aquí 👇\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14bcb724-d298-4a88-981f-a12b005ddd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Explicacion del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e886848-0374-4dba-8894-0ccb81b476a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. **Parsed Logical Plan**\n",
    "    ```rust\n",
    "    'Aggregate ['`%`('id_squared, 10)], [unresolvedalias(...)]\n",
    "    +- 'Project ...\n",
    "    +- 'Filter ...\n",
    "        +- Range ...\n",
    "    ```\n",
    "- Aquí Spark acaba de leer tu código (todavía con símbolos `'` y unresolvedalias → significa que no resolvió los nombres ni los tipos).\n",
    "- Es básicamente un \"parseo\" del SQL/DataFrame API.\n",
    "- Es la traducción literal de tu código en objetos internos de Spark.\n",
    "\n",
    "2. **Analyzed Logical Plan**\n",
    "    ```rust\n",
    "    Aggregate [(id_squared#... % cast(10 as bigint))], ...\n",
    "    +- Project [id, (id * id) AS id_squared]\n",
    "    +- Filter (id % 2 = 0)\n",
    "        +- Range (0, 10000000, step=1, splits=Some(8))\n",
    "    ```\n",
    "- Ahora Spark ya sabe los tipos de datos y las columnas reales.\n",
    "- `id_squared` ya no está \"unresolved\", aparece como `id_squared#11044L`.\n",
    "- Tu `count(1)` ya está identificado como `bigint`.\n",
    "- Es la versión \"semánticamente válida\" del plan.\n",
    "\n",
    "3. **Optimized Logical Plan**\n",
    "    ```rust\n",
    "    Aggregate [_groupingexpression#11046L], ...\n",
    "    +- Project [((id * id) % 10) AS _groupingexpression#11046L]\n",
    "    +- Filter (id % 2 = 0)\n",
    "        +- Range ...\n",
    "    ```\n",
    "- Aquí Spark aplica optimizaciones lógicas (Catalyst optimizer).\n",
    "- Ejemplo: en lugar de calcular id_squared y luego % 10, Spark ya fusionó eso en un solo paso: ((id * id) % 10) → _groupingexpression.\n",
    "- Esto reduce trabajo intermedio.\n",
    "- Es un plan más eficiente antes de pasar a lo físico.\n",
    "\n",
    "4. **Physical Plan**\n",
    "    ```rust\n",
    "AdaptiveSparkPlan isFinalPlan=false\n",
    "+- PhotonGroupingAgg\n",
    "   +- PhotonShuffleExchangeSource\n",
    "      +- PhotonShuffleMapStage\n",
    "         +- PhotonShuffleExchangeSink hashpartitioning(_groupingexpression, 1024)\n",
    "            +- PhotonGroupingAgg (partial)\n",
    "               +- PhotonProject ...\n",
    "                  +- PhotonFilter ...\n",
    "                     +- PhotonRange ...\n",
    "    ```\n",
    "    Aquí está lo bueno\n",
    "- `PhotonRange` → genera los datos iniciales (0 a 10M).\n",
    "- `PhotonFilter` → filtra pares.\n",
    "- `PhotonProject` → calcula `(id*id) % 10`.\n",
    "- `PhotonGroupingAgg (partial)` → primer aggregation parcial, cada partición cuenta localmente.\n",
    "- `PhotonShuffleExchangeSink hashpartitioning(..., 1024)` → 🔥 AQUÍ está el shuffle.\n",
    "- Spark reorganiza los datos en 1024 particiones para agrupar por la clave id_squared % 10.\n",
    "- `PhotonShuffleMapStage` + `PhotonShuffleExchangeSource` → representan el movimiento de datos entre nodos.\n",
    "- `PhotonGroupingAgg (final)` → combina los resultados de cada partición tras el shuffle.\n",
    "- `ColumnarToRow` al final → convierte de formato columnar interno a filas, para que lo entienda el driver.\n",
    "- Este es el plan real que ejecuta el cluster, y donde verás cuellos de botella (el shuffle).\n",
    "\n",
    "5. **Photon Explanation**\n",
    "    ```rust\n",
    "    The query is fully supported by Photon.\n",
    "    ```\n",
    "- Significa que todo el pipeline puede ejecutarse con Photon (engine vectorizado en C++ de Databricks).\n",
    "- Es mucho más rápido que la ejecución normal en la JVM.\n",
    "- Si hubiera partes no soportadas, Photon te avisaría.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f5a9a4-af6a-4779-957b-daefd121035f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Resumen didactico\n",
    "- **Parsed plan**: Spark solo entendió la sintaxis.\n",
    "- **Analyzed plan**: Spark ya entiende las columnas y tipos.\n",
    "- **Optimized plan**: Spark reorganiza el trabajo para hacerlo más eficiente.\n",
    "- **Physical plan**: cómo se ejecuta realmente (con shuffle, stages, etc.).\n",
    "- **Photon explanation**: confirmación de que Photon lo acelera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3249f571-e82c-4790-bbad-2891c81fe399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 2 – DAG, stages y tasks\n",
    "  Spark divide los pipelines en DAGs → stages → tasks.\n",
    "  \n",
    "  Entender esto nos hace comprender cómo afecta al paralelismo y en definitiva al rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe983023-4ad0-4937-bbf0-5ebb4e67e9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. **DAG (Directed Acyclic Graph)**\n",
    "   - Qué es: Representación del flujo de transformaciones como un grafo dirigido sin ciclos.\n",
    "    - Cada nodo = operación (map, filter, join, etc.).\n",
    "    - Cada arista = dependencia de datos entre operaciones.\n",
    "    - Spark construye el DAG de manera perezosa (lazy evaluation).\n",
    "    - El DAG no se ejecuta hasta que se encuentra una acción (`.show()`, `.collect()`, `.count()`).\n",
    "\n",
    "2. **Stages**\n",
    "   - El DAG se divide en stages según los puntos de shuffle.\n",
    "   - Narrow dependency: cada partición depende solo de una partición anterior → mismo stage. Ejemplo: `map`, `filter`.\n",
    "   - Wide dependency: una partición depende de varias → nuevo stage. Ejemplo: `groupBy`, `join`, `distinct`.\n",
    "\n",
    "3. **Tasks**\n",
    "   - La unidad más pequeña de trabajo en Spark.\n",
    "   - Cada task procesa una partición de datos en un executor.\n",
    "   - Ejemplo: si tengo 200 particiones → Spark lanza 200 tasks (distribuidas en los executors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa590f9-67e8-43f4-b6a2-e6a157668db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo DAG Complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029fb563-70f4-4a20-9f19-b5c4439e63b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "# Ajustamos las particiones de shuffle para que se generen muchas tasks\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "# ==============================\n",
    "# DataFrame base (50 millones filas, 200 particiones iniciales)\n",
    "# ==============================\n",
    "df = spark.range(0, 50_000_000, numPartitions=200).withColumn(\"value\", (col(\"id\") * rand()))\n",
    "\n",
    "# ==============================\n",
    "# JOB 1: count con 2 stages\n",
    "# ==============================\n",
    "# Stage 1: lectura + filtro (narrow)\n",
    "df_filtered = df.filter(col(\"value\") > 1000)\n",
    "\n",
    "# Stage 2: shuffle por groupBy + count\n",
    "df_grouped = df_filtered.groupBy((col(\"id\") % 10).alias(\"bucket\")).count()\n",
    "\n",
    "# Acción que dispara Job 1\n",
    "print(\"Job 1 result:\", df_grouped.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3d8fab-0091-498e-9ba8-6d853f799b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0fd1500-68f8-4c20-a45a-932d5d7b1361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a show(), Spark vuelve a ejecutar el pipeline porque es una acción.\n",
    "# Esta vez el job tiene 2 stages (lectura + shuffle para mostrar los datos),\n",
    "# en vez de 3 como en el count, porque show() no necesita contar todas las filas, solo mostrar algunas.\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b13d6a-97e2-4887-b583-96ba7b8258db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58a6f9c-abdd-46e9-b2a0-d4cf47c0d5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a cache(), Spark vuelve a ejecutar el pipeline porque es una acción.\n",
    "# al igual que antes el job tiene 2 stages pero el segundo más largo con la instrucción de cache al final\n",
    "\n",
    "df_grouped.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f91c6e4-9092-4d54-8ac1-ee76172aaeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b29302d-4678-4e42-bcc4-627f35befa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a show(), Spark lee de la cache el df y eso ahorra tiempo\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f465ab-55df-4e0a-a702-8ba24121343e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bceef465-3f44-43d0-8fa7-95aefcdc58cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# JOB 2: join con 3 stages\n",
    "# ==============================\n",
    "df1 = spark.range(0, 10_000_000, numPartitions=100).withColumn(\"k\", col(\"id\") % 5000)\n",
    "df2 = spark.range(0, 20_000_000, numPartitions=150).withColumn(\"k\", col(\"id\") % 5000)\n",
    "\n",
    "# Stage 1: lectura df1\n",
    "# Stage 2: lectura df2\n",
    "# Stage 3: shuffle para join y acción final (show)\n",
    "df_joined = df1.join(df2, on=\"k\", how=\"inner\")\n",
    "\n",
    "# Acción que dispara Job 2\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4da337-540f-4d97-8b22-6f11f316d7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Job1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00078903-9b5a-47be-9308-e0dd3b2bca68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Stage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c968591a-5d36-401c-841c-504f3e91e2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Stage2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b2e2caa-4b7c-41c4-a611-69624a00df03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "   **Ejercicio 2**\n",
    "  1. Generar un DataFrame con `spark.range(0, 10000000)`.\n",
    "  2. Aplicar transformaciones simples (filter, withColumn).\n",
    "  3. Comparar dos queries:\n",
    "     - `df.groupBy(\"id\").count()`\n",
    "     - `df.filter(\"id % 2 = 0\").count()`\n",
    "\n",
    "  4. Preguntas:\n",
    "     - ¿Cuál introduce un shuffle?\n",
    "     - ¿Cómo se refleja en el `explain(\"extended\")`?\n",
    "     - ¿En cuál esperas más stages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285fdb10-9ebb-4fff-aaa6-47d623b4881a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu código aquí 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8fa909-201c-4c8b-96ea-903fa8b635fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Ejercicios tipo test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f060a4-07bc-4817-8b8a-8ec7aa5b910e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**1. DAG**\n",
    "\n",
    "**Pregunta:**\n",
    "**¿Cuándo se construye el DAG en Spark?**\n",
    "\n",
    "a) Cuando se arranca el cluster.\n",
    "\n",
    "b) Cada vez que escribimos una transformación (filter, map, etc.).\n",
    "\n",
    "c) Solo cuando ejecutamos una acción (show, count, collect).\n",
    "\n",
    "d) Después de terminar todos los jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586e38d0-85ec-4063-93cd-abf2759de95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**2. Jobs**\n",
    "\n",
    "**Pregunta:**\n",
    "**En Spark, ¿qué dispara la creación de un nuevo job?**\n",
    "\n",
    "a) Un withColumn.\n",
    "\n",
    "b) Una acción como .show() o .count().\n",
    "\n",
    "c) Cada vez que se aplica un filtro.\n",
    "\n",
    "d) Cada vez que se crea un DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d336099c-f73d-4815-b44f-a40dc0aef819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**3. Stages**\n",
    "\n",
    "**Pregunta:\n",
    "¿Qué provoca que Spark divida un job en varios stages?**\n",
    "\n",
    "a) Cada 1000 filas procesadas.\n",
    "\n",
    "b) La presencia de un filter.\n",
    "\n",
    "c) La presencia de una operación con wide dependency (por ejemplo, groupBy, join, orderBy).\n",
    "\n",
    "d) Cada vez que se seleccionan nuevas columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032a9a3a-6710-4646-97f9-77fbd833b850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**4. Tasks**\n",
    "\n",
    "**Pregunta:\n",
    "Si un DataFrame tiene 200 particiones y hacemos un count(), ¿cuántas tasks se lanzan en el stage correspondiente?**\n",
    "\n",
    "a) 1\n",
    "\n",
    "b) 200\n",
    "\n",
    "c) Depende de los executors\n",
    "\n",
    "d) Ninguna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c712bdbb-ee0c-4923-a1ab-4bd5fac5f553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**5. explain()**\n",
    "\n",
    "**Pregunta:\n",
    "¿Qué muestra el comando .explain(True) en PySpark?**\n",
    "\n",
    "a) Solo el resultado de la consulta.\n",
    "\n",
    "b) El DAG gráfico.\n",
    "\n",
    "c) El plan lógico (parsed, analyzed, optimized) y el plan físico que Spark planea ejecutar.\n",
    "\n",
    "d) El número de executors activos en el cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4338bc7c-9e9b-4b7c-aab8-9495dc132f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**6. Spark UI**\n",
    "\n",
    "**Pregunta:\n",
    "¿Cuál es la diferencia entre abrir la Spark UI desde un notebook o desde la pestaña Compute en Databricks?**\n",
    "\n",
    "a) Ninguna, siempre se ve lo mismo.\n",
    "\n",
    "b) Desde el notebook se ve solo la aplicación de ese notebook; desde Compute se ve la lista de todas las aplicaciones Spark activas en el cluster.\n",
    "\n",
    "c) Desde Compute solo se ven jobs, no stages.\n",
    "\n",
    "d) Desde el notebook se ven todos los notebooks del cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c767c9e5-be27-4036-9d7f-a0ccb0613c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## **Ejercicios prácticos (predicción de Jobs, Stages y Tasks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7525d42-e168-4208-9909-e2df4cc2395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Count sin shuffle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ca41a8-c9f4-43f6-875b-1acb98cae7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d17732-30ff-47d8-9751-07764ebdb26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¿Cuántos jobs se disparan?\n",
    "- ¿Cuántos stages tendrá el job principal?\n",
    "- ¿Cuántas tasks por stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d644b7-b52b-46e3-a177-eb65b7b10f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bccb9ca0-da9c-4138-ba14-b555af0ea30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**2. Filtros encadenados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef4bb9e-8088-4553-bc28-6b48516558a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=16)\n",
    "df.filter(\"id > 1000\").filter(\"id < 900000\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c21735-b14f-4f20-a3e1-e12e2c704eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¿Qué pasa con los filtros? ¿Se ven varios en el plan físico?\n",
    "- ¿Cuántos jobs/stages/tasks esperas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a0119f-0b0d-4b32-9d12-9526dd4e5e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26fbdbfb-9792-488c-9c85-8bac80568404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**3. Agregación (wide dependency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45f3acf-6a9e-4090-a549-6187926e0c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8)\n",
    "df.groupBy((df.id % 4).alias(\"cat\")).count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28410c05-4c81-4d22-b4ee-4de3471c580b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "- ¿Cuántos stages habrá y por qué?\n",
    "- ¿Cuántas tasks esperas en el reduce? (pista: depende de spark.sql.shuffle.partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb81789-3028-4167-9567-402fde46d9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "108aa538-236c-4f9d-9c22-c78ba4e6bbee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**4. Order + limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d7559f-6837-49fb-ad81-81d5a29c2fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8).withColumn(\"v\", (df.id % 100))\n",
    "df.orderBy(\"v\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429dde86-6d49-4bef-af4f-932c452a0398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¿Cuántos stages aparecen?\n",
    "- ¿Qué pasa con las particiones en el último stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73bd5cb-ebab-4e46-8f29-dde205762db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145ad671-bfcb-4584-bd5f-14ec7512017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**5. Dos acciones consecutivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb64e804-cc97-41bd-9679-7913d1f819f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=4)\n",
    "df.filter(\"id < 1000\").count()\n",
    "df.filter(\"id > 500000\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f7d78b3-9510-43e8-bfdd-f32d39a6ae37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¿Cuántos jobs en total?\n",
    "- ¿Se reutilizan stages entre las dos acciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d4c157-57f3-454e-98f1-71f0eb8b4200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1129486b-1764-48ef-896b-530b714062fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 3 – Shuffles, Data Skew y Particiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259b2549-c489-4b6a-90e6-a6bfc20101d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffles\n",
    "  - Redistribución de datos entre particiones.\n",
    "  - Se producen en `groupBy`, `join`, `distinct`, `orderBy`.\n",
    "  - Costosos porque implican:\n",
    "    - Escritura a disco (shuffle files).\n",
    "    - Transferencia por red.\n",
    "    - Lectura posterior.\n",
    "  - ⚠️ Los shuffles son la principal fuente de cuellos de botella en Spark.\n",
    "\n",
    "### Data Skew (desequilibrio de datos) \n",
    "  - Cuando una clave concentra muchos más datos que el resto.\n",
    "  - Consecuencia: tareas desbalanceadas → unas rápidas, otras muy lentas.\n",
    "  - Efecto visible: long tail tasks.\n",
    "  - Ejemplo: 80% de las filas en la misma clave.\n",
    "\n",
    "### Particiones y paralelismo\n",
    "  - Spark divide el trabajo en particiones → tasks.\n",
    "  - Demasiadas particiones → overhead administrativo, demasiados archivos pequeños.\n",
    "  - Muy pocas particiones → subutilización de CPU, tareas gigantes que bloquean.\n",
    "  - Config clave:\n",
    "    - `spark.sql.shuffle.partitions` (por defecto: 200).\n",
    "    - `repartition()` y `coalesce()`.\n",
    "\n",
    "  \n",
    "  El rendimiento depende de encontrar el equilibrio justo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b93d25-6b4b-4eaa-ac90-69a7be81f09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4969f150-b8fd-4f52-9879-b74de6ed77d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setup particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace96c20-bcec-4f4e-837c-ef8d418541cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, rand\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")  # valor inicial por defecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c19cc2c-33e5-47fa-bf41-c05fd880bc4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Shuffle grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a1e161-ac98-48ba-a670-b8e480d9adb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210d5c37-3570-45b3-a187-2f1975b0bba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generamos un DF con 100M filas y 1M claves\n",
    "df = spark.range(0, 100_000_000).withColumn(\"key\", (col(\"id\") % 1000000))\n",
    "\n",
    "# Disparar un shuffle con groupBy\n",
    "res = df.groupBy(\"key\").count()\n",
    "res.explain(\"extended\")   # 👉 Fíjate en el Exchange hashpartitioning\n",
    "res.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97dfa352-6aea-458b-8abe-8c95d84b6e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observa:\n",
    "- Aparece un Exchange (hashpartitioning) en el plan.\n",
    "- Esto implica escritura/lectura de shuffle files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60dd1589-f9c9-47f8-b830-76ec793896a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caac26e3-2bd9-449c-9284-6f050cb55881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Forzar muchas particiones en el shuffle\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "\n",
    "N = 10_000_000\n",
    "df = spark.range(0, N).withColumn(\n",
    "    \"skewed_key\",\n",
    "    when(col(\"id\") < int(N*0.99), lit(1))   # 95% de las filas van con la clave \"1\"\n",
    "    .otherwise((col(\"id\") % 1000) + 2)      # el 5% restante se reparte\n",
    ")\n",
    "\n",
    "df_grouped = df.groupBy(\"skewed_key\").count()\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e63904-9d1d-4065-b485-70cd7721b186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Data Skew KO](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Data skew ko.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56a9480-5f1a-4e07-ae3c-d67632c1ab1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observa:\n",
    "- Una sola clave (key = 0) concentra la mayoría de las filas.\n",
    "- En la ejecución, unas tareas terminan rápido y otras tardan muchísimo (long tail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa6846f-dd09-4c3d-8ae1-6660662e91f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Data Skew OK](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Data skew ok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce43416b-84a3-4557-bf9d-13a5c61f2fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejemplo de ajustes de particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7bb3ed3-a1f1-49b7-a3e0-fb9db23874bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Probar distintos valores de particiones\n",
    "for p in [10, 200, 2000]:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", p)\n",
    "    print(f\"\\n=== Shuffle partitions: {p} ===\")\n",
    "    res = df.groupBy(\"key\").count()\n",
    "    res.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278a6023-ec01-40d0-b50c-3aaa793e2a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Con 10 → pocas tasks, CPU infrautilizada.\n",
    "- Con 2000 → overhead, demasiados archivos pequeños.\n",
    "- Con 200 → equilibrio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8869a99c-e919-40e8-afff-650b4c0864d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63dc609-82bd-40b0-b013-dbb557315792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 1 – Shuffles**\n",
    "- Usa `explain(\"extended\")` sobre `df.groupBy(\"key\").count()`.\n",
    "- Identifica dónde aparece el Exchange.\n",
    "- Cambia el número de particiones (`spark.sql.shuffle.partitions`) y repite.\n",
    "- Pregunta: ¿cuántos stages aparecen con 10, 200 y 2000 particiones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b7278a-f074-4c62-8f08-c9c16207ab85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748c8f8d-ed7b-4583-869b-305106baa506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 2 – Skew en acción**\n",
    "- Genera un DataFrame con skew (80% de los datos en la misma clave).\n",
    "- Ejecuta `groupBy(\"key\").count()` y mide tiempos.\n",
    "- Mira en el plan físico → ¿cómo se refleja el shuffle?\n",
    "- Pregunta: ¿qué pasa con las tareas asociadas a la clave dominante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000cad78-3af5-4023-99f4-4af48ca8f56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e170a1c-4b60-4350-9f82-dae9f19b4d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para comparar sin skew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e370e056-01ec-4245-baec-f6f2899526b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e73cdee-9036-4495-8086-955b89884bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ejercicio Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6880a24e-6980-4d4b-b9cd-49cf77709ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Contexto: eres el equipo de datos de una aerolínea. Cada noche calculáis KPIs operativos sobre 100M vuelos. El informe llega tarde. Tenéis que diagnosticar y optimizar el pipeline de “retraso medio por aeropuerto y compañía”.\n",
    "\n",
    "El objetivo es optimizar este trozo de código para optimizar el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605bd480-a988-4238-8f2f-faaafda97f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, expr, col\n",
    "\n",
    "N = 10_000_000_000\n",
    "flights = (spark.range(N)\n",
    "           .withColumn(\"airport\", expr(\"CASE WHEN rand() < 0.7 THEN 'JFK' WHEN rand() < 0.15 THEN 'LAX' ELSE 'ORD' END\"))\n",
    "           .withColumn(\"carrier\", expr(\"CASE id % 5 WHEN 0 THEN 'AA' WHEN 1 THEN 'UA' WHEN 2 THEN 'DL' WHEN 3 THEN 'SW' ELSE 'IB' END\"))\n",
    "           .withColumn(\"delay_min\", (rand()*180).cast(\"int\")))\n",
    "\n",
    "# Versión inicial (no optimizada)\n",
    "kpis0 = flights.groupBy(\"airport\",\"carrier\").avg(\"delay_min\")\n",
    "kpis0.explain(\"extended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e520ba0-85a8-4544-9deb-d5ee9ab65eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Qué esperar: Exchange `hashpartitioning((airport, carrier), N)`, y tiempo alto por skew en `JFK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840692f6-a957-4303-9995-5ff0a522242c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 1. Lo primero es diagnosticar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a441070-88b9-4779-9574-780368bbf27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprobar distribución\n",
    "\n",
    "# Probar distintos suhffle partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7cabe7-5ef3-4cd3-8a2b-9ea5f1426453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 2. Optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1f8376-f5ff-424b-b054-19424b9fec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reparticionar por clave antes de agregar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1089cb00-231d-40b5-ac8d-922002c539f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 3. Responder a estas preguntas\n",
    " - ¿Dónde estaba el shuffle? ¿Cómo lo viste?\n",
    " - ¿Cómo detectaste el skew sin Spark UI?\n",
    " - ¿Qué valor de spark.sql.shuffle.partitions te dio mejor resultado en tu clúster?\n",
    " - ¿Cuánto mejoró el tiempo con repartition/salting? (anotar segundos antes/después)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c8b09a-1e03-48e4-bccc-72882a3cb291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8ce693-6a9a-49c8-9e59-bfba11804b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780a2531-dbf1-4219-9e78-5e38a7a9a358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25ee2bf-c25b-486b-8b95-a32a831f1ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimizacion - Sesion 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
