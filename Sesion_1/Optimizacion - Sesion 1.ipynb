{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd33e414-79ec-44a8-a8ec-01fcf7e6fad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#   Clase 1 ‚Äì Fundamentos y diagn√≥stico de rendimiento\n",
    "  **Objetivo:** Comprender c√≥mo Spark ejecuta el c√≥digo y aprender a diagnosticar cuellos de botella.\n",
    "  \n",
    "  Estructura:\n",
    "  1. Arquitectura y ejecuci√≥n (driver, executors, DAG)\n",
    "  2. Stages y tasks\n",
    "  3. Shuffles y data skew\n",
    "  4. Particiones y paralelismo\n",
    "  5. Herramientas de diagn√≥stico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c6f37ed-656a-4181-a0dd-fa0018c07077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 1 ‚Äì Transformaciones vs Acciones\n",
    "  Spark es *lazy*: las transformaciones no ejecutan nada hasta que llega una acci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c4c370c-5934-4cc0-b5da-259f59c79fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformaciones\n",
    "- Operaciones que definen un nuevo DataFrame/RDD.\n",
    "- _Lazy evaluation_: no ejecutan nada hasta que llega una acci√≥n.\n",
    "- Son operaciones perezosas (_lazy evaluation_): no se ejecutan inmediatamente.\n",
    "- Devuelven un nuevo DataFrame/RDD con un plan de ejecuci√≥n actualizado, pero sin lanzar un job.\n",
    "- Se ejecutan solo cuando se dispara una acci√≥n.\n",
    "- Pueden ser de dos tipos:\n",
    "    1. Narrow transformations: los datos de una partici√≥n se usan solo en esa misma partici√≥n.\n",
    "        - Ejemplos: `map()`, `filter()`, `select()`, `withColumn()`.\n",
    "        - M√°s eficientes (no requieren shuffle).\n",
    "    2. Wide transformations: requieren mover datos entre particiones ‚Üí shuffle.\n",
    "        - Ejemplos: `groupBy()`, `join()`, `distinct()`, `repartition()`.\n",
    "        - Costosas en tiempo y recursos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23a67a3b-c889-4ae0-8519-f4ff1306c5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.range(1, 100).withColumn(\"x2\", col(\"id\") * 2)  # transformaci√≥n\n",
    "df_filtered = df.filter(col(\"id\") > 50)                   # transformaci√≥n\n",
    "\n",
    "# Hasta aqu√≠ no se ejecuta nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57236584-d785-4546-ae7a-c860e329c367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Acciones:\n",
    "- Disparan la ejecuci√≥n real del plan.\n",
    "- Devuelven un resultado al driver o escriben datos en almacenamiento.\n",
    "- Ejemplos:\n",
    "  - `collect()` ‚Üí trae todos los datos al driver (‚ö†Ô∏è cuidado con Out Of Memory).\n",
    "  - `show()` ‚Üí muestra primeras filas en consola.\n",
    "  - `count()` ‚Üí cuenta filas.\n",
    "  - `write.format(\"delta\").save(...)` ‚Üí guarda en disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0ed58d-1760-40fa-85f5-5117903bff8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filtered.show()   # aqu√≠ Spark ejecuta el pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f0f6f8-8236-41cf-a6d1-9bddb138860a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23153387-c2a0-4ed9-84ef-f72f2713144e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformaciones (lazy)\n",
    "df = spark.range(1, 1000000)\n",
    "df2 = df.withColumn(\"x2\", col(\"id\") * 2)   # narrow\n",
    "df3 = df2.filter(col(\"x2\") % 5 == 0)       # narrow\n",
    "df4 = df3.groupBy((col(\"x2\") % 10)).count() # wide ‚Üí shuffle\n",
    "\n",
    "# Acci√≥n (trigger)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8cea157-43aa-445f-ba4a-8dfe703dbe5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Resultado:\n",
    "- Hasta `.groupBy()` solo hay transformaciones ‚Üí Spark construye el DAG.\n",
    "- Al llamar `.show()`, Spark ejecuta el job, lo divide en stages y tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c91bf1d-2381-472a-9b80-ad06a1cc5449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark UI y `explain()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c75ffce4-51b4-40d6-9f1b-e0784b7d72f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark UI\n",
    "La Spark UI es la interfaz web que te muestra qu√© est√° pasando dentro de tu aplicaci√≥n Spark. En Databricks puedes acceder desde la pesta√±a Spark UI de cada job o notebook.\n",
    "Los elementos principales son:\n",
    "1. Jobs tab\n",
    "   - Lista de trabajos disparados por acciones (count(), show(), write...).\n",
    "   - Cada job corresponde a la ejecuci√≥n de un DAG.\n",
    "   - Desde aqu√≠ entras a Stages.\n",
    "2. Stages tab\n",
    "   - Muestra c√≥mo se divide el job en stages (fases).\n",
    "   - Cada stage tiene m√∫ltiples tasks (una por partici√≥n).\n",
    "   - M√©tricas clave: tiempo, duraci√≥n, skew, GC, I/O.\n",
    "3. Tasks tab (dentro de un stage)\n",
    "   - Detalle de cada partici√≥n ejecutada.\n",
    "   - Puedes detectar data skew: si una task tarda mucho m√°s que las dem√°s.\n",
    "   - M√©tricas de input size, shuffle read/write, memoria usada.\n",
    "4. SQL tab\n",
    "   - Muy √∫til si usas DataFrames/SQL.\n",
    "   - Visualiza el plan l√≥gico y f√≠sico en forma de √°rbol.\n",
    "   - Identifica d√≥nde ocurren shuffles y scans de tablas. \n",
    "5. Storage tab\n",
    "   - Muestra qu√© DataFrames/RDDs est√°n cacheados.\n",
    "   - Permite ver el uso de memoria y disco para persistencia.\n",
    "\n",
    "En Databricks tambi√©n tienes el DAG Viewer, un grafo visual que muestra stages y dependencias ‚Üí muy √∫til para ense√±ar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42874a24-c0d6-4454-b9ad-568a61e5093b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Spark UI Jobs](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/SparkUI Jobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01df16a-de83-43f8-a323-ce75cbcb89a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![SparkUI Stages](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/SparkUI Stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f83fc06b-740b-4aee-97d9-26d7ffa65fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### `explained()`\n",
    "El m√©todo `.explain()` muestra c√≥mo Spark planea ejecutar tu DataFrame.\n",
    "Opciones principales:\n",
    "- `.explain()` ‚Üí plan f√≠sico resumido.\n",
    "- `.explain(\"extended\")` ‚Üí plan l√≥gico + optimizado + f√≠sico.\n",
    "- `.explain(\"cost\")` ‚Üí incluye estimaci√≥n de costes (filas, bytes).\n",
    "- `.explain(\"formatted\")` ‚Üí salida legible en tabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b731f7b7-7fc8-40ef-8c09-f4018091d1a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo Practico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5529aebf-79d3-4567-8377-b8b8587cf5db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Crear DataFrame grande\n",
    "df = spark.range(0, 10000000)\n",
    "\n",
    "# Solo definimos transformaciones (lazy)\n",
    "df_filtered = df.filter(df.id % 2 == 0)\n",
    "df_transformed = df_filtered.withColumn(\"id_squared\", df.id * df.id)\n",
    "\n",
    "# Ver plan l√≥gico/f√≠sico\n",
    "df_transformed.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d55ed70-1d09-4327-963e-8bf278047910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso a paso en el ejemplo:\n",
    "1. PhotonRange (1)\n",
    "   - Spark crea un dataset de enteros (de 0 a 10,000,000).\n",
    "   - Est√° dividido en 8 splits (= particiones iniciales).\n",
    "   - Paralelizaci√≥n inicial.\n",
    "2. PhotonFilter (2)\n",
    "   - Filtra solo los n√∫meros pares: (id % 2 = 0).\n",
    "   - Es un narrow transformation (no hay shuffle).\n",
    "3. PhotonProject (3)\n",
    "   - Calcula una nueva columna id_squared.\n",
    "   - Otra transformaci√≥n ligera, todav√≠a sin shuffle.\n",
    "4. PhotonResultStage (4)\n",
    "   - Prepara los resultados para pasarlos al driver.\n",
    "5. ColumnarToRow (5)\n",
    "   - Convierte los datos del formato columnar interno (usado por Photon) a filas normales, porque el driver no entiende el formato columnar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe6e4cb3-8f91-43c8-95d4-1b57b19d7ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ahora ejecutamos una acci√≥n\n",
    "df_transformed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66df874e-91e5-426b-8b77-900913bb24b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicio 1\n",
    "  1. Genera un DataFrame con 10 millones de filas.\n",
    "  2. Aplica un `filter` y un `groupBy` con `count`.\n",
    "  3. Usa `.explain(\"extended\")` para inspeccionar el plan.\n",
    "  \n",
    "  ‚ùì Pregunta: ¬øen qu√© momento se ejecuta la query realmente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8ffffc-e5c6-47a1-94d8-3f8fc3eef8c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ üëá\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14bcb724-d298-4a88-981f-a12b005ddd54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Explicacion del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e886848-0374-4dba-8894-0ccb81b476a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. **Parsed Logical Plan**\n",
    "    ```rust\n",
    "    'Aggregate ['`%`('id_squared, 10)], [unresolvedalias(...)]\n",
    "    +- 'Project ...\n",
    "    +- 'Filter ...\n",
    "        +- Range ...\n",
    "    ```\n",
    "- Aqu√≠ Spark acaba de leer tu c√≥digo (todav√≠a con s√≠mbolos `'` y unresolvedalias ‚Üí significa que no resolvi√≥ los nombres ni los tipos).\n",
    "- Es b√°sicamente un \"parseo\" del SQL/DataFrame API.\n",
    "- Es la traducci√≥n literal de tu c√≥digo en objetos internos de Spark.\n",
    "\n",
    "2. **Analyzed Logical Plan**\n",
    "    ```rust\n",
    "    Aggregate [(id_squared#... % cast(10 as bigint))], ...\n",
    "    +- Project [id, (id * id) AS id_squared]\n",
    "    +- Filter (id % 2 = 0)\n",
    "        +- Range (0, 10000000, step=1, splits=Some(8))\n",
    "    ```\n",
    "- Ahora Spark ya sabe los tipos de datos y las columnas reales.\n",
    "- `id_squared` ya no est√° \"unresolved\", aparece como `id_squared#11044L`.\n",
    "- Tu `count(1)` ya est√° identificado como `bigint`.\n",
    "- Es la versi√≥n \"sem√°nticamente v√°lida\" del plan.\n",
    "\n",
    "3. **Optimized Logical Plan**\n",
    "    ```rust\n",
    "    Aggregate [_groupingexpression#11046L], ...\n",
    "    +- Project [((id * id) % 10) AS _groupingexpression#11046L]\n",
    "    +- Filter (id % 2 = 0)\n",
    "        +- Range ...\n",
    "    ```\n",
    "- Aqu√≠ Spark aplica optimizaciones l√≥gicas (Catalyst optimizer).\n",
    "- Ejemplo: en lugar de calcular id_squared y luego % 10, Spark ya fusion√≥ eso en un solo paso: ((id * id) % 10) ‚Üí _groupingexpression.\n",
    "- Esto reduce trabajo intermedio.\n",
    "- Es un plan m√°s eficiente antes de pasar a lo f√≠sico.\n",
    "\n",
    "4. **Physical Plan**\n",
    "    ```rust\n",
    "AdaptiveSparkPlan isFinalPlan=false\n",
    "+- PhotonGroupingAgg\n",
    "   +- PhotonShuffleExchangeSource\n",
    "      +- PhotonShuffleMapStage\n",
    "         +- PhotonShuffleExchangeSink hashpartitioning(_groupingexpression, 1024)\n",
    "            +- PhotonGroupingAgg (partial)\n",
    "               +- PhotonProject ...\n",
    "                  +- PhotonFilter ...\n",
    "                     +- PhotonRange ...\n",
    "    ```\n",
    "    Aqu√≠ est√° lo bueno\n",
    "- `PhotonRange` ‚Üí genera los datos iniciales (0 a 10M).\n",
    "- `PhotonFilter` ‚Üí filtra pares.\n",
    "- `PhotonProject` ‚Üí calcula `(id*id) % 10`.\n",
    "- `PhotonGroupingAgg (partial)` ‚Üí primer aggregation parcial, cada partici√≥n cuenta localmente.\n",
    "- `PhotonShuffleExchangeSink hashpartitioning(..., 1024)` ‚Üí üî• AQU√ç est√° el shuffle.\n",
    "- Spark reorganiza los datos en 1024 particiones para agrupar por la clave id_squared % 10.\n",
    "- `PhotonShuffleMapStage` + `PhotonShuffleExchangeSource` ‚Üí representan el movimiento de datos entre nodos.\n",
    "- `PhotonGroupingAgg (final)` ‚Üí combina los resultados de cada partici√≥n tras el shuffle.\n",
    "- `ColumnarToRow` al final ‚Üí convierte de formato columnar interno a filas, para que lo entienda el driver.\n",
    "- Este es el plan real que ejecuta el cluster, y donde ver√°s cuellos de botella (el shuffle).\n",
    "\n",
    "5. **Photon Explanation**\n",
    "    ```rust\n",
    "    The query is fully supported by Photon.\n",
    "    ```\n",
    "- Significa que todo el pipeline puede ejecutarse con Photon (engine vectorizado en C++ de Databricks).\n",
    "- Es mucho m√°s r√°pido que la ejecuci√≥n normal en la JVM.\n",
    "- Si hubiera partes no soportadas, Photon te avisar√≠a.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9f5a9a4-af6a-4779-957b-daefd121035f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Resumen didactico\n",
    "- **Parsed plan**: Spark solo entendi√≥ la sintaxis.\n",
    "- **Analyzed plan**: Spark ya entiende las columnas y tipos.\n",
    "- **Optimized plan**: Spark reorganiza el trabajo para hacerlo m√°s eficiente.\n",
    "- **Physical plan**: c√≥mo se ejecuta realmente (con shuffle, stages, etc.).\n",
    "- **Photon explanation**: confirmaci√≥n de que Photon lo acelera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3249f571-e82c-4790-bbad-2891c81fe399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 2 ‚Äì DAG, stages y tasks\n",
    "  Spark divide los pipelines en DAGs ‚Üí stages ‚Üí tasks.\n",
    "  \n",
    "  Entender esto nos hace comprender c√≥mo afecta al paralelismo y en definitiva al rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe983023-4ad0-4937-bbf0-5ebb4e67e9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. **DAG (Directed Acyclic Graph)**\n",
    "   - Qu√© es: Representaci√≥n del flujo de transformaciones como un grafo dirigido sin ciclos.\n",
    "    - Cada nodo = operaci√≥n (map, filter, join, etc.).\n",
    "    - Cada arista = dependencia de datos entre operaciones.\n",
    "    - Spark construye el DAG de manera perezosa (lazy evaluation).\n",
    "    - El DAG no se ejecuta hasta que se encuentra una acci√≥n (`.show()`, `.collect()`, `.count()`).\n",
    "\n",
    "2. **Stages**\n",
    "   - El DAG se divide en stages seg√∫n los puntos de shuffle.\n",
    "   - Narrow dependency: cada partici√≥n depende solo de una partici√≥n anterior ‚Üí mismo stage. Ejemplo: `map`, `filter`.\n",
    "   - Wide dependency: una partici√≥n depende de varias ‚Üí nuevo stage. Ejemplo: `groupBy`, `join`, `distinct`.\n",
    "\n",
    "3. **Tasks**\n",
    "   - La unidad m√°s peque√±a de trabajo en Spark.\n",
    "   - Cada task procesa una partici√≥n de datos en un executor.\n",
    "   - Ejemplo: si tengo 200 particiones ‚Üí Spark lanza 200 tasks (distribuidas en los executors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faa590f9-67e8-43f4-b6a2-e6a157668db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo DAG Complejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "029fb563-70f4-4a20-9f19-b5c4439e63b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "# Ajustamos las particiones de shuffle para que se generen muchas tasks\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "\n",
    "# ==============================\n",
    "# DataFrame base (50 millones filas, 200 particiones iniciales)\n",
    "# ==============================\n",
    "df = spark.range(0, 50_000_000, numPartitions=200).withColumn(\"value\", (col(\"id\") * rand()))\n",
    "\n",
    "# ==============================\n",
    "# JOB 1: count con 2 stages\n",
    "# ==============================\n",
    "# Stage 1: lectura + filtro (narrow)\n",
    "df_filtered = df.filter(col(\"value\") > 1000)\n",
    "\n",
    "# Stage 2: shuffle por groupBy + count\n",
    "df_grouped = df_filtered.groupBy((col(\"id\") % 10).alias(\"bucket\")).count()\n",
    "\n",
    "# Acci√≥n que dispara Job 1\n",
    "print(\"Job 1 result:\", df_grouped.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3d8fab-0091-498e-9ba8-6d853f799b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0fd1500-68f8-4c20-a45a-932d5d7b1361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a show(), Spark vuelve a ejecutar el pipeline porque es una acci√≥n.\n",
    "# Esta vez el job tiene 2 stages (lectura + shuffle para mostrar los datos),\n",
    "# en vez de 3 como en el count, porque show() no necesita contar todas las filas, solo mostrar algunas.\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06b13d6a-97e2-4887-b583-96ba7b8258db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f58a6f9c-abdd-46e9-b2a0-d4cf47c0d5eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a cache(), Spark vuelve a ejecutar el pipeline porque es una acci√≥n.\n",
    "# al igual que antes el job tiene 2 stages pero el segundo m√°s largo con la instrucci√≥n de cache al final\n",
    "\n",
    "df_grouped.cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f91c6e4-9092-4d54-8ac1-ee76172aaeeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b29302d-4678-4e42-bcc4-627f35befa87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cuando llamamos a show(), Spark lee de la cache el df y eso ahorra tiempo\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0f465ab-55df-4e0a-a702-8ba24121343e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej1 - Job4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bceef465-3f44-43d0-8fa7-95aefcdc58cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# JOB 2: join con 3 stages\n",
    "# ==============================\n",
    "df1 = spark.range(0, 10_000_000, numPartitions=100).withColumn(\"k\", col(\"id\") % 5000)\n",
    "df2 = spark.range(0, 20_000_000, numPartitions=150).withColumn(\"k\", col(\"id\") % 5000)\n",
    "\n",
    "# Stage 1: lectura df1\n",
    "# Stage 2: lectura df2\n",
    "# Stage 3: shuffle para join y acci√≥n final (show)\n",
    "df_joined = df1.join(df2, on=\"k\", how=\"inner\")\n",
    "\n",
    "# Acci√≥n que dispara Job 2\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c4da337-540f-4d97-8b22-6f11f316d7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Job1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00078903-9b5a-47be-9308-e0dd3b2bca68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Stage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c968591a-5d36-401c-841c-504f3e91e2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Ej2 - Stage2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b2e2caa-4b7c-41c4-a611-69624a00df03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "   **Ejercicio 2**\n",
    "  1. Generar un DataFrame con `spark.range(0, 10000000)`.\n",
    "  2. Aplicar transformaciones simples (filter, withColumn).\n",
    "  3. Comparar dos queries:\n",
    "     - `df.groupBy(\"id\").count()`\n",
    "     - `df.filter(\"id % 2 = 0\").count()`\n",
    "\n",
    "  4. Preguntas:\n",
    "     - ¬øCu√°l introduce un shuffle?\n",
    "     - ¬øC√≥mo se refleja en el `explain(\"extended\")`?\n",
    "     - ¬øEn cu√°l esperas m√°s stages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285fdb10-9ebb-4fff-aaa6-47d623b4881a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a8fa909-201c-4c8b-96ea-903fa8b635fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Ejercicios tipo test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f060a4-07bc-4817-8b8a-8ec7aa5b910e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**1. DAG**\n",
    "\n",
    "**Pregunta:**\n",
    "**¬øCu√°ndo se construye el DAG en Spark?**\n",
    "\n",
    "a) Cuando se arranca el cluster.\n",
    "\n",
    "b) Cada vez que escribimos una transformaci√≥n (filter, map, etc.).\n",
    "\n",
    "c) Solo cuando ejecutamos una acci√≥n (show, count, collect).\n",
    "\n",
    "d) Despu√©s de terminar todos los jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "586e38d0-85ec-4063-93cd-abf2759de95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**2. Jobs**\n",
    "\n",
    "**Pregunta:**\n",
    "**En Spark, ¬øqu√© dispara la creaci√≥n de un nuevo job?**\n",
    "\n",
    "a) Un withColumn.\n",
    "\n",
    "b) Una acci√≥n como .show() o .count().\n",
    "\n",
    "c) Cada vez que se aplica un filtro.\n",
    "\n",
    "d) Cada vez que se crea un DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d336099c-f73d-4815-b44f-a40dc0aef819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**3. Stages**\n",
    "\n",
    "**Pregunta:\n",
    "¬øQu√© provoca que Spark divida un job en varios stages?**\n",
    "\n",
    "a) Cada 1000 filas procesadas.\n",
    "\n",
    "b) La presencia de un filter.\n",
    "\n",
    "c) La presencia de una operaci√≥n con wide dependency (por ejemplo, groupBy, join, orderBy).\n",
    "\n",
    "d) Cada vez que se seleccionan nuevas columnas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "032a9a3a-6710-4646-97f9-77fbd833b850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**4. Tasks**\n",
    "\n",
    "**Pregunta:\n",
    "Si un DataFrame tiene 200 particiones y hacemos un count(), ¬øcu√°ntas tasks se lanzan en el stage correspondiente?**\n",
    "\n",
    "a) 1\n",
    "\n",
    "b) 200\n",
    "\n",
    "c) Depende de los executors\n",
    "\n",
    "d) Ninguna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c712bdbb-ee0c-4923-a1ab-4bd5fac5f553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**5. explain()**\n",
    "\n",
    "**Pregunta:\n",
    "¬øQu√© muestra el comando .explain(True) en PySpark?**\n",
    "\n",
    "a) Solo el resultado de la consulta.\n",
    "\n",
    "b) El DAG gr√°fico.\n",
    "\n",
    "c) El plan l√≥gico (parsed, analyzed, optimized) y el plan f√≠sico que Spark planea ejecutar.\n",
    "\n",
    "d) El n√∫mero de executors activos en el cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4338bc7c-9e9b-4b7c-aab8-9495dc132f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**6. Spark UI**\n",
    "\n",
    "**Pregunta:\n",
    "¬øCu√°l es la diferencia entre abrir la Spark UI desde un notebook o desde la pesta√±a Compute en Databricks?**\n",
    "\n",
    "a) Ninguna, siempre se ve lo mismo.\n",
    "\n",
    "b) Desde el notebook se ve solo la aplicaci√≥n de ese notebook; desde Compute se ve la lista de todas las aplicaciones Spark activas en el cluster.\n",
    "\n",
    "c) Desde Compute solo se ven jobs, no stages.\n",
    "\n",
    "d) Desde el notebook se ven todos los notebooks del cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c767c9e5-be27-4036-9d7f-a0ccb0613c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## **Ejercicios pr√°cticos (predicci√≥n de Jobs, Stages y Tasks)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7525d42-e168-4208-9909-e2df4cc2395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**1. Count sin shuffle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ca41a8-c9f4-43f6-875b-1acb98cae7b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8)\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61d17732-30ff-47d8-9751-07764ebdb26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¬øCu√°ntos jobs se disparan?\n",
    "- ¬øCu√°ntos stages tendr√° el job principal?\n",
    "- ¬øCu√°ntas tasks por stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d644b7-b52b-46e3-a177-eb65b7b10f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bccb9ca0-da9c-4138-ba14-b555af0ea30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**2. Filtros encadenados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef4bb9e-8088-4553-bc28-6b48516558a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=16)\n",
    "df.filter(\"id > 1000\").filter(\"id < 900000\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82c21735-b14f-4f20-a3e1-e12e2c704eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¬øQu√© pasa con los filtros? ¬øSe ven varios en el plan f√≠sico?\n",
    "- ¬øCu√°ntos jobs/stages/tasks esperas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a0119f-0b0d-4b32-9d12-9526dd4e5e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26fbdbfb-9792-488c-9c85-8bac80568404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**3. Agregaci√≥n (wide dependency)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d45f3acf-6a9e-4090-a549-6187926e0c11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8)\n",
    "df.groupBy((df.id % 4).alias(\"cat\")).count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28410c05-4c81-4d22-b4ee-4de3471c580b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "- ¬øCu√°ntos stages habr√° y por qu√©?\n",
    "- ¬øCu√°ntas tasks esperas en el reduce? (pista: depende de spark.sql.shuffle.partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccb81789-3028-4167-9567-402fde46d9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "108aa538-236c-4f9d-9c22-c78ba4e6bbee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**4. Order + limit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d7559f-6837-49fb-ad81-81d5a29c2fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=8).withColumn(\"v\", (df.id % 100))\n",
    "df.orderBy(\"v\").limit(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429dde86-6d49-4bef-af4f-932c452a0398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¬øCu√°ntos stages aparecen?\n",
    "- ¬øQu√© pasa con las particiones en el √∫ltimo stage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73bd5cb-ebab-4e46-8f29-dde205762db3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "145ad671-bfcb-4584-bd5f-14ec7512017d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "**5. Dos acciones consecutivas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb64e804-cc97-41bd-9679-7913d1f819f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 1_000_000, 1, numPartitions=4)\n",
    "df.filter(\"id < 1000\").count()\n",
    "df.filter(\"id > 500000\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f7d78b3-9510-43e8-bfdd-f32d39a6ae37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Pregunta:\n",
    "\n",
    "- ¬øCu√°ntos jobs en total?\n",
    "- ¬øSe reutilizan stages entre las dos acciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d4c157-57f3-454e-98f1-71f0eb8b4200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1129486b-1764-48ef-896b-530b714062fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bloque 3 ‚Äì Shuffles, Data Skew y Particiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259b2549-c489-4b6a-90e6-a6bfc20101d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Shuffles\n",
    "  - Redistribuci√≥n de datos entre particiones.\n",
    "  - Se producen en `groupBy`, `join`, `distinct`, `orderBy`.\n",
    "  - Costosos porque implican:\n",
    "    - Escritura a disco (shuffle files).\n",
    "    - Transferencia por red.\n",
    "    - Lectura posterior.\n",
    "  - ‚ö†Ô∏è Los shuffles son la principal fuente de cuellos de botella en Spark.\n",
    "\n",
    "### Data Skew (desequilibrio de datos) \n",
    "  - Cuando una clave concentra muchos m√°s datos que el resto.\n",
    "  - Consecuencia: tareas desbalanceadas ‚Üí unas r√°pidas, otras muy lentas.\n",
    "  - Efecto visible: long tail tasks.\n",
    "  - Ejemplo: 80% de las filas en la misma clave.\n",
    "\n",
    "### Particiones y paralelismo\n",
    "  - Spark divide el trabajo en particiones ‚Üí tasks.\n",
    "  - Demasiadas particiones ‚Üí overhead administrativo, demasiados archivos peque√±os.\n",
    "  - Muy pocas particiones ‚Üí subutilizaci√≥n de CPU, tareas gigantes que bloquean.\n",
    "  - Config clave:\n",
    "    - `spark.sql.shuffle.partitions` (por defecto: 200).\n",
    "    - `repartition()` y `coalesce()`.\n",
    "\n",
    "  \n",
    "  El rendimiento depende de encontrar el equilibrio justo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b93d25-6b4b-4eaa-ac90-69a7be81f09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4969f150-b8fd-4f52-9879-b74de6ed77d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setup particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace96c20-bcec-4f4e-837c-ef8d418541cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, rand\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2000\")  # valor inicial por defecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c19cc2c-33e5-47fa-bf41-c05fd880bc4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Shuffle grande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6a1e161-ac98-48ba-a670-b8e480d9adb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210d5c37-3570-45b3-a187-2f1975b0bba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generamos un DF con 100M filas y 1M claves\n",
    "df = spark.range(0, 100_000_000).withColumn(\"key\", (col(\"id\") % 1000000))\n",
    "\n",
    "# Disparar un shuffle con groupBy\n",
    "res = df.groupBy(\"key\").count()\n",
    "res.explain(\"extended\")   # üëâ F√≠jate en el Exchange hashpartitioning\n",
    "res.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97dfa352-6aea-458b-8abe-8c95d84b6e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observa:\n",
    "- Aparece un Exchange (hashpartitioning) en el plan.\n",
    "- Esto implica escritura/lectura de shuffle files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60dd1589-f9c9-47f8-b830-76ec793896a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caac26e3-2bd9-449c-9284-6f050cb55881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Forzar muchas particiones en el shuffle\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "\n",
    "N = 10_000_000\n",
    "df = spark.range(0, N).withColumn(\n",
    "    \"skewed_key\",\n",
    "    when(col(\"id\") < int(N*0.99), lit(1))   # 95% de las filas van con la clave \"1\"\n",
    "    .otherwise((col(\"id\") % 1000) + 2)      # el 5% restante se reparte\n",
    ")\n",
    "\n",
    "df_grouped = df.groupBy(\"skewed_key\").count()\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e63904-9d1d-4065-b485-70cd7721b186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Data Skew KO](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Data skew ko.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d56a9480-5f1a-4e07-ae3c-d67632c1ab1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observa:\n",
    "- Una sola clave (key = 0) concentra la mayor√≠a de las filas.\n",
    "- En la ejecuci√≥n, unas tareas terminan r√°pido y otras tardan much√≠simo (long tail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fa6846f-dd09-4c3d-8ae1-6660662e91f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![Data Skew OK](/Workspace/Users/psanzc@hotmail.com/optimizacion_databricks/Fotos/Data skew ok.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce43416b-84a3-4557-bf9d-13a5c61f2fa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejemplo de ajustes de particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7bb3ed3-a1f1-49b7-a3e0-fb9db23874bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Probar distintos valores de particiones\n",
    "for p in [10, 200, 2000]:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", p)\n",
    "    print(f\"\\n=== Shuffle partitions: {p} ===\")\n",
    "    res = df.groupBy(\"key\").count()\n",
    "    res.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278a6023-ec01-40d0-b50c-3aaa793e2a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Con 10 ‚Üí pocas tasks, CPU infrautilizada.\n",
    "- Con 2000 ‚Üí overhead, demasiados archivos peque√±os.\n",
    "- Con 200 ‚Üí equilibrio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8869a99c-e919-40e8-afff-650b4c0864d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63dc609-82bd-40b0-b013-dbb557315792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 1 ‚Äì Shuffles**\n",
    "- Usa `explain(\"extended\")` sobre `df.groupBy(\"key\").count()`.\n",
    "- Identifica d√≥nde aparece el Exchange.\n",
    "- Cambia el n√∫mero de particiones (`spark.sql.shuffle.partitions`) y repite.\n",
    "- Pregunta: ¬øcu√°ntos stages aparecen con 10, 200 y 2000 particiones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10b7278a-f074-4c62-8f08-c9c16207ab85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "748c8f8d-ed7b-4583-869b-305106baa506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Ejercicio 2 ‚Äì Skew en acci√≥n**\n",
    "- Genera un DataFrame con skew (80% de los datos en la misma clave).\n",
    "- Ejecuta `groupBy(\"key\").count()` y mide tiempos.\n",
    "- Mira en el plan f√≠sico ‚Üí ¬øc√≥mo se refleja el shuffle?\n",
    "- Pregunta: ¬øqu√© pasa con las tareas asociadas a la clave dominante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000cad78-3af5-4023-99f4-4af48ca8f56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e170a1c-4b60-4350-9f82-dae9f19b4d3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para comparar sin skew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e370e056-01ec-4245-baec-f6f2899526b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tu codigo aqui\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e73cdee-9036-4495-8086-955b89884bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ejercicio Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6880a24e-6980-4d4b-b9cd-49cf77709ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Contexto: eres el equipo de datos de una aerol√≠nea. Cada noche calcul√°is KPIs operativos sobre 100M vuelos. El informe llega tarde. Ten√©is que diagnosticar y optimizar el pipeline de ‚Äúretraso medio por aeropuerto y compa√±√≠a‚Äù.\n",
    "\n",
    "El objetivo es optimizar este trozo de c√≥digo para optimizar el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605bd480-a988-4238-8f2f-faaafda97f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, expr, col\n",
    "\n",
    "N = 10_000_000_000\n",
    "flights = (spark.range(N)\n",
    "           .withColumn(\"airport\", expr(\"CASE WHEN rand() < 0.7 THEN 'JFK' WHEN rand() < 0.15 THEN 'LAX' ELSE 'ORD' END\"))\n",
    "           .withColumn(\"carrier\", expr(\"CASE id % 5 WHEN 0 THEN 'AA' WHEN 1 THEN 'UA' WHEN 2 THEN 'DL' WHEN 3 THEN 'SW' ELSE 'IB' END\"))\n",
    "           .withColumn(\"delay_min\", (rand()*180).cast(\"int\")))\n",
    "\n",
    "# Versi√≥n inicial (no optimizada)\n",
    "kpis0 = flights.groupBy(\"airport\",\"carrier\").avg(\"delay_min\")\n",
    "kpis0.explain(\"extended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e520ba0-85a8-4544-9deb-d5ee9ab65eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Qu√© esperar: Exchange `hashpartitioning((airport, carrier), N)`, y tiempo alto por skew en `JFK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "840692f6-a957-4303-9995-5ff0a522242c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 1. Lo primero es diagnosticar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a441070-88b9-4779-9574-780368bbf27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comprobar distribuci√≥n\n",
    "\n",
    "# Probar distintos suhffle partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e7cabe7-5ef3-4cd3-8a2b-9ea5f1426453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 2. Optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1f8376-f5ff-424b-b054-19424b9fec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reparticionar por clave antes de agregar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1089cb00-231d-40b5-ac8d-922002c539f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paso 3. Responder a estas preguntas\n",
    " - ¬øD√≥nde estaba el shuffle? ¬øC√≥mo lo viste?\n",
    " - ¬øC√≥mo detectaste el skew sin Spark UI?\n",
    " - ¬øQu√© valor de spark.sql.shuffle.partitions te dio mejor resultado en tu cl√∫ster?\n",
    " - ¬øCu√°nto mejor√≥ el tiempo con repartition/salting? (anotar segundos antes/despu√©s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c8b09a-1e03-48e4-bccc-72882a3cb291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8ce693-6a9a-49c8-9e59-bfba11804b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "780a2531-dbf1-4219-9e78-5e38a7a9a358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25ee2bf-c25b-486b-8b95-a32a831f1ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimizacion - Sesion 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
