{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68940fe7-9903-43a2-a852-3a422ebcf0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sesion 3 - DiseÃ±o de pipelines escalables y sostenibles en Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f51234f8-9c91-438c-87e4-4d0b6dd968a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objetivo\n",
    "\n",
    "Construir un pipeline completo en Databricks (bronce â†’ plata â†’ oro), orquestado con Workflows, parametrizado con configuraciÃ³n externa, con controles de calidad, ejemplo de optimizaciÃ³n de rendimiento y un pipeline CI/CD bÃ¡sico.\n",
    "\n",
    "## Parte 1 â€“ PreparaciÃ³n del entorno\n",
    "1. Descargar el proyecto\n",
    "    - Descarga el archivo ZIP proporcionado: `airline-pipeline-practica.zip`.\n",
    "    - DescomprÃ­melo en tu ordenador.\n",
    "2. Subirlo a Databricks\n",
    "    - Entra en tu workspace â†’ pestaÃ±a Repos.\n",
    "    - Crea un nuevo repo o conecta tu GitHub.\n",
    "    - Sube la carpeta descomprimida completa.\n",
    "3. Verificar estructura\n",
    "    - Debes ver carpetas como: `conf/`, `data/`, `src/`, `workflows/`.\n",
    "\n",
    "## Parte 2 â€“ ConfiguraciÃ³n inicial\n",
    "ğŸ‘‰ Ejercicio 1 â€“ Crear catÃ¡logo y esquemas\n",
    "  - Abre el notebook `00_setup.py`.\n",
    "  - EjecÃºtalo.\n",
    "  - Verifica que ahora tienes:\n",
    "    - CatÃ¡logo: `airline_demo`\n",
    "    - Esquemas: `bronze`, `silver`, `gold`\n",
    "\n",
    "## Parte 3 â€“ Ingesta (Bronze)\n",
    "ğŸ‘‰ Ejercicio 2 â€“ Ingestar vuelos\n",
    "  - Abre `10_bronze_ingest_flights.py`.\n",
    "  - EjecÃºtalo y confirma que se creÃ³ la tabla `bronze.flights`.\n",
    "ğŸ‘‰ Ejercicio 3 â€“ Ingestar aeropuertos\n",
    "  - Abre `11_bronze_ingest_airports.py`.\n",
    "  - EjecÃºtalo y confirma que existe `bronze.airports`.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© ventajas tiene mantener las ingestas sin limpiar en Bronze?\n",
    "\n",
    "## Parte 4 â€“ Transformaciones (Silver)\n",
    "ğŸ‘‰ Ejercicio 4 â€“ Limpiar y enriquecer\n",
    "  - Abre `20_silver_transform.py`.\n",
    "  - Revisa el cÃ³digo: convierte `delay` en entero y agrega ciudad de origen.\n",
    "  - Ejecuta el notebook â†’ confirma tabla `silver.flights_enriched`.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© problemas puede haber si los tipos de datos no se normalizan aquÃ­?\n",
    "\n",
    "## Parte 5 â€“ AnalÃ­tica (Gold)\n",
    "ğŸ‘‰ Ejercicio 5 â€“ Reporte de demoras\n",
    "  - Abre `30_gold_analytics.py`.\n",
    "  - Ejecuta el notebook â†’ confirma tabla `gold.delay_summary`.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© tipo de dashboards/consumidores podrÃ­an usar estas tablas oro?\n",
    "\n",
    "## Parte 6 â€“ Calidad de Datos\n",
    "\n",
    "ğŸ‘‰ Ejercicio 6 â€“ Chequeo de calidad\n",
    "  - Abre `40_quality_and_metrics.py`.\n",
    "  - Ejecuta: validarÃ¡ que la columna `delay` no tenga nulos.\n",
    "  - Prueba inyectar un nulo en la tabla y vuelve a correr el check para ver el error.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿Por quÃ© es importante â€œfallar rÃ¡pidoâ€ cuando detectamos un dato invÃ¡lido?\n",
    "\n",
    "## Parte 7 â€“ OptimizaciÃ³n de rendimiento\n",
    "ğŸ‘‰ Ejercicio 7 â€“ DiagnÃ³stico y tuning\n",
    "  - Abre `50_performance_optimization.py`.\n",
    "  - Ejecuta la versiÃ³n â€œlentaâ€ del join â†’ observa el `explain`.\n",
    "  - Activa AQE + broadcast join â†’ observa el tiempo de ejecuciÃ³n.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© diferencia hay entre `cache` y `checkpoint` en Spark?\n",
    "\n",
    "## Parte 8 â€“ OrquestaciÃ³n con Workflows\n",
    "ğŸ‘‰ Ejercicio 8 â€“ Crear un Job\n",
    "  - Ve a Workflows â†’ Jobs â†’ Create Job.\n",
    "  - Importa el YAML `workflows/job_airline_pipeline.yaml`.\n",
    "  - Revisa las tareas encadenadas.\n",
    "  - Corre el Job completo y observa cÃ³mo se ejecuta cada fase.\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© ventajas tiene usar Workflows en lugar de lanzar notebooks manualmente?\n",
    "\n",
    "## Parte 9 â€“ CI/CD\n",
    "ğŸ‘‰ Ejercicio 9 â€“ Deploy automÃ¡tico\n",
    "  - Revisa el archivo `ci/github-actions-deploy.yml`.\n",
    "  - Observa que:\n",
    "    - Se instala el CLI de Databricks.\n",
    "    - Usa secrets (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`).\n",
    "    - Despliega el workflow con cada push a main.\n",
    "  - Para hacerlo funcionar hay que hacer lo siguiente_\n",
    "    - Crear un entorno y `github actions` y crear los `secrets`\n",
    "    - Modificar el id del job\n",
    "\n",
    "â“ Pregunta:\n",
    "Â¿QuÃ© riesgo evitamos al desplegar vÃ­a CI/CD en lugar de hacerlo manualmente?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimizacion - Sesion 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
