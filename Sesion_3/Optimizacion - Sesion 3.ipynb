{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68940fe7-9903-43a2-a852-3a422ebcf0ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Sesion 3 - Diseño de pipelines escalables y sostenibles en Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f51234f8-9c91-438c-87e4-4d0b6dd968a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Objetivo\n",
    "\n",
    "Construir un pipeline completo en Databricks (bronce → plata → oro), orquestado con Workflows, parametrizado con configuración externa, con controles de calidad, ejemplo de optimización de rendimiento y un pipeline CI/CD básico.\n",
    "\n",
    "## Parte 1 – Preparación del entorno\n",
    "1. Descargar el proyecto\n",
    "    - Descarga el archivo ZIP proporcionado: `airline-pipeline-practica.zip`.\n",
    "    - Descomprímelo en tu ordenador.\n",
    "2. Subirlo a Databricks\n",
    "    - Entra en tu workspace → pestaña Repos.\n",
    "    - Crea un nuevo repo o conecta tu GitHub.\n",
    "    - Sube la carpeta descomprimida completa.\n",
    "3. Verificar estructura\n",
    "    - Debes ver carpetas como: `conf/`, `data/`, `src/`, `workflows/`.\n",
    "\n",
    "## Parte 2 – Configuración inicial\n",
    "👉 Ejercicio 1 – Crear catálogo y esquemas\n",
    "  - Abre el notebook `00_setup.py`.\n",
    "  - Ejecútalo.\n",
    "  - Verifica que ahora tienes:\n",
    "    - Catálogo: `airline_demo`\n",
    "    - Esquemas: `bronze`, `silver`, `gold`\n",
    "\n",
    "## Parte 3 – Ingesta (Bronze)\n",
    "👉 Ejercicio 2 – Ingestar vuelos\n",
    "  - Abre `10_bronze_ingest_flights.py`.\n",
    "  - Ejecútalo y confirma que se creó la tabla `bronze.flights`.\n",
    "👉 Ejercicio 3 – Ingestar aeropuertos\n",
    "  - Abre `11_bronze_ingest_airports.py`.\n",
    "  - Ejecútalo y confirma que existe `bronze.airports`.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué ventajas tiene mantener las ingestas sin limpiar en Bronze?\n",
    "\n",
    "## Parte 4 – Transformaciones (Silver)\n",
    "👉 Ejercicio 4 – Limpiar y enriquecer\n",
    "  - Abre `20_silver_transform.py`.\n",
    "  - Revisa el código: convierte `delay` en entero y agrega ciudad de origen.\n",
    "  - Ejecuta el notebook → confirma tabla `silver.flights_enriched`.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué problemas puede haber si los tipos de datos no se normalizan aquí?\n",
    "\n",
    "## Parte 5 – Analítica (Gold)\n",
    "👉 Ejercicio 5 – Reporte de demoras\n",
    "  - Abre `30_gold_analytics.py`.\n",
    "  - Ejecuta el notebook → confirma tabla `gold.delay_summary`.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué tipo de dashboards/consumidores podrían usar estas tablas oro?\n",
    "\n",
    "## Parte 6 – Calidad de Datos\n",
    "\n",
    "👉 Ejercicio 6 – Chequeo de calidad\n",
    "  - Abre `40_quality_and_metrics.py`.\n",
    "  - Ejecuta: validará que la columna `delay` no tenga nulos.\n",
    "  - Prueba inyectar un nulo en la tabla y vuelve a correr el check para ver el error.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Por qué es importante “fallar rápido” cuando detectamos un dato inválido?\n",
    "\n",
    "## Parte 7 – Optimización de rendimiento\n",
    "👉 Ejercicio 7 – Diagnóstico y tuning\n",
    "  - Abre `50_performance_optimization.py`.\n",
    "  - Ejecuta la versión “lenta” del join → observa el `explain`.\n",
    "  - Activa AQE + broadcast join → observa el tiempo de ejecución.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué diferencia hay entre `cache` y `checkpoint` en Spark?\n",
    "\n",
    "## Parte 8 – Orquestación con Workflows\n",
    "👉 Ejercicio 8 – Crear un Job\n",
    "  - Ve a Workflows → Jobs → Create Job.\n",
    "  - Importa el YAML `workflows/job_airline_pipeline.yaml`.\n",
    "  - Revisa las tareas encadenadas.\n",
    "  - Corre el Job completo y observa cómo se ejecuta cada fase.\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué ventajas tiene usar Workflows en lugar de lanzar notebooks manualmente?\n",
    "\n",
    "## Parte 9 – CI/CD\n",
    "👉 Ejercicio 9 – Deploy automático\n",
    "  - Revisa el archivo `ci/github-actions-deploy.yml`.\n",
    "  - Observa que:\n",
    "    - Se instala el CLI de Databricks.\n",
    "    - Usa secrets (`DATABRICKS_HOST`, `DATABRICKS_TOKEN`).\n",
    "    - Despliega el workflow con cada push a main.\n",
    "  - Para hacerlo funcionar hay que hacer lo siguiente_\n",
    "    - Crear un entorno y `github actions` y crear los `secrets`\n",
    "    - Modificar el id del job\n",
    "\n",
    "❓ Pregunta:\n",
    "¿Qué riesgo evitamos al desplegar vía CI/CD en lugar de hacerlo manualmente?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimizacion - Sesion 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
